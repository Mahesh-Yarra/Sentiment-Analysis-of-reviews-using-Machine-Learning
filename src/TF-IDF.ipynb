{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dependencies to be downloaded**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r ../requirements.txt\n",
        "# %pip install kaggle\n",
        "# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p ../Dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import zipfile\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from urllib.parse import urlsplit\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Set Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths for Local Run\n",
        "IMDB_PREPROCESSED = r\"../Datasets/IMDB_PREPROCESSED.csv\"\n",
        "TFIDF_FEATURES = r\"../Datasets/TFIDF_FEATURES.csv\"\n",
        "LEXICON_POSITIVE = r\"../Datasets/positive-words.txt\"\n",
        "LEXICON_NEGATIVE = r\"../Datasets/negative-words.txt\"\n",
        "LEXICON_CONNOTATION = r\"../Datasets/connotations.csv\"\n",
        "POSNEG_CONN_FEATURES = r\"../Datasets/POSNEG_CONN_FEATURES.csv\"\n",
        "POSNEG_FEATURES = r\"../Datasets/POSNEG_FEATURES.csv\"\n",
        "VADER_SCORES_FEATURES = r\"../Datasets/VADER_SCORES_FEATURES.csv\"\n",
        "TARGET_VALUES = r\"../Datasets/TARGET_VALUES.csv\"\n",
        "\n",
        "INFOGAIN_TFIDF_1K = r\"../Datasets/INFOGAIN_TFIDF_1K.csv\"\n",
        "INFOGAIN_TFIDF_2K = r\"../Datasets/INFOGAIN_TFIDF_2K.csv\"\n",
        "INFOGAIN_TFIDF_3K = r\"../Datasets/INFOGAIN_TFIDF_3K.csv\"\n",
        "INFOGAIN_TFIDF_4K = r\"../Datasets/INFOGAIN_TFIDF_4K.csv\"\n",
        "INFOGAIN_TFIDF_5K = r\"../Datasets/INFOGAIN_TFIDF_5K.csv\"\n",
        "INFOGAIN_TFIDF_6K = r\"../Datasets/INFOGAIN_TFIDF_6K.csv\"\n",
        "INFOGAIN_TFIDF_7K = r\"../Datasets/INFOGAIN_TFIDF_7K.csv\"\n",
        "INFOGAIN_TFIDF_8K = r\"../Datasets/INFOGAIN_TFIDF_8K.csv\"\n",
        "\n",
        "CHI_TFIDF_1K = r\"../Datasets/CHI_TFIDF_1K.csv\"\n",
        "CHI_TFIDF_2K = r\"../Datasets/CHI_TFIDF_2K.csv\"\n",
        "CHI_TFIDF_3K = r\"../Datasets/CHI_TFIDF_3K.csv\"\n",
        "CHI_TFIDF_4K = r\"../Datasets/CHI_TFIDF_4K.csv\"\n",
        "CHI_TFIDF_5K = r\"../Datasets/CHI_TFIDF_5K.csv\"\n",
        "CHI_TFIDF_6K = r\"../Datasets/CHI_TFIDF_6K.csv\"\n",
        "CHI_TFIDF_7K = r\"../Datasets/CHI_TFIDF_7K.csv\"\n",
        "CHI_TFIDF_8K = r\"../Datasets/CHI_TFIDF_8K.csv\"\n",
        "\n",
        "CORR_TFIDF_1K = r\"../Datasets/CORR_TFIDF_1K.csv\"\n",
        "CORR_TFIDF_2K = r\"../Datasets/CORR_TFIDF_2K.csv\"\n",
        "CORR_TFIDF_3K = r\"../Datasets/CORR_TFIDF_3K.csv\"\n",
        "CORR_TFIDF_4K = r\"../Datasets/CORR_TFIDF_4K.csv\"\n",
        "CORR_TFIDF_5K = r\"../Datasets/CORR_TFIDF_5K.csv\"\n",
        "CORR_TFIDF_6K = r\"../Datasets/CORR_TFIDF_6K.csv\"\n",
        "CORR_TFIDF_7K = r\"../Datasets/CORR_TFIDF_7K.csv\"\n",
        "CORR_TFIDF_8K = r\"../Datasets/CORR_TFIDF_8K.csv\"\n",
        "\n",
        "LDA_150 = r\"../Datasets/LDA_150.csv\"\n",
        "LDA_200 = r\"../Datasets/LDA_200.csv\"\n",
        "LDA_250 = r\"../Datasets/LDA_250.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataset Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to the ZIP file\n",
        "zip_file_path = r\"../Datasets/imdb-dataset-of-50k-movie-reviews.zip\"\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Find the first file with a .csv extension (assuming it's the one you want)\n",
        "    csv_file = [name for name in zip_ref.namelist() if name.endswith('.csv')][0]\n",
        "    \n",
        "    # Read the CSV file directly from the ZIP archive into a DataFrame\n",
        "    df = pd.read_csv(zip_ref.open(csv_file))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset = df.sample(n=5000,random_state=42).reset_index(drop=True)\n",
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lowercase\n",
        "df_subset[\"review\"]=df_subset[\"review\"].apply(lambda x:x.lower())\n",
        "\n",
        "#Remove punctuations\n",
        "df_subset['review'] = df_subset['review'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "\n",
        "# Remove numbers from the 'reviews' column\n",
        "df_subset['review'] = df_subset['review'].str.replace(r'\\d+', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove stopwords from a text\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'review' column\n",
        "df_subset['review'] = df_subset['review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    # Define a regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Find all matches in the text\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    # Remove URLs from the text\n",
        "    text_without_urls = re.sub(url_pattern, '', text)\n",
        "\n",
        "    return text_without_urls\n",
        "\n",
        "# Example usage\n",
        "df_subset['review'] = df_subset['review'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**HTML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "df_subset[\"review\"] = df_subset[\"review\"].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non-Alphanumeric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extra spaces**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_extra_whitespaces(text):\n",
        "    # Use regular expression to replace multiple whitespaces with a single space\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(remove_extra_whitespaces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Filter non-English comments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_non_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Create a boolean mask for non-English reviews\n",
        "mask = df_subset['review'].apply(filter_non_english)\n",
        "\n",
        "# Create a new DataFrame containing only English reviews\n",
        "df_subset = df_subset[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lemma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to get the part of speech for WordNet lemmatizer\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if the part of speech is not found\n",
        "\n",
        "# Function to lemmatize a text\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply lemmatization to the 'text' column\n",
        "df_subset['review'] = df_subset['review'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LabelEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label = LabelEncoder()\n",
        "df_subset['sentiment'] = label.fit_transform(df_subset['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_subset.to_csv('../Datasets/IMDB_Preprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **2. Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = pd.read_csv(IMDB_PREPROCESSED)\n",
        "preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "totalFeatures = vectorizer.fit_transform(preprocessed['review'])\n",
        "column_names = vectorizer.get_feature_names_out()\n",
        "totalFeatures = pd.DataFrame(totalFeatures.toarray(), columns=column_names)\n",
        "\n",
        "totalFeatures.to_csv(TFIDF_FEATURES, index=False)\n",
        "\n",
        "totalFeatures.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Connotations Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connotations = pd.read_csv(LEXICON_CONNOTATION)\n",
        "\n",
        "word_emotion_map = dict(zip(connotations['word'], connotations['emotion']))\n",
        "\n",
        "def update_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'positive')\n",
        "    negative_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'negative')\n",
        "    return positive_count, negative_count\n",
        "\n",
        "pos_neg_conn_counts_df = preprocessed['review'].apply(lambda x: pd.Series(update_counts(x), index=['Positive_Connotation_Count', 'Negative_Connotation_Count']))\n",
        "\n",
        "pos_neg_conn_counts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store the Connotations Count Features\n",
        "pos_neg_conn_counts_df.to_csv(POSNEG_CONN_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Positive and Negative Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load positive and negative words from files\n",
        "positive_words_df = pd.read_csv(LEXICON_POSITIVE, header=None, names=['words'])\n",
        "negative_words_df = pd.read_csv(LEXICON_NEGATIVE, header=None, names=['words'])\n",
        "\n",
        "# Convert DataFrame columns to sets\n",
        "positive_words = set(positive_words_df['words'].tolist())\n",
        "negative_words = set(negative_words_df['words'].tolist())\n",
        "\n",
        "# Define a function to update counts based on positive and negative words\n",
        "def update_word_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in positive_words)\n",
        "    negative_count = sum(1 for word in review.split() if word in negative_words)\n",
        "    return positive_count, negative_count\n",
        "\n",
        "pos_neg_counts_df = preprocessed['review'].apply(lambda x: pd.Series(update_word_counts(x), index=['Positive_Word_Count', 'Negative_Word_Count']))\n",
        "pos_neg_counts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_neg_counts_df.to_csv(POSNEG_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d. Vader Positive Score and Negative Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Use VADER for sentiment analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(review):\n",
        "    scores = sid.polarity_scores(review)\n",
        "    return scores['pos'] *100, scores['neg'] * 100\n",
        "\n",
        "vader_scores_df = preprocessed['review'].apply(lambda x: pd.Series(vader_sentiment(x), index=['Positive_VADER_Count', 'Negative_VADER_Count']))\n",
        "vader_scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vader_scores_df.to_csv(VADER_SCORES_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e. Topic Modeling (Latent Dirichlet Allocation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# Define the range of number of topics\n",
        "num_topics_range = [150, 200, 250]  # Add more values if needed\n",
        "\n",
        "# Define the file paths\n",
        "file_paths = {\n",
        "    150: LDA_150,\n",
        "    200: LDA_200,\n",
        "    250: LDA_250\n",
        "}\n",
        "\n",
        "# Loop through different numbers of topics\n",
        "for num_topics in num_topics_range:\n",
        "    \n",
        "    lda_pipeline = make_pipeline(\n",
        "        CountVectorizer(),  # CountVectorizer converts text to a matrix of token counts\n",
        "        TfidfTransformer(),  # TF-IDF transformation\n",
        "        LatentDirichletAllocation(n_components=num_topics, random_state=42)  # LDA for topic modeling\n",
        "    )\n",
        "    \n",
        "    # Fit and transform data using the LDA pipeline\n",
        "    X_lda = lda_pipeline.fit_transform(preprocessed['review'])\n",
        "    \n",
        "    # Create DataFrame for LDA features\n",
        "    X_lda = pd.DataFrame(X_lda, columns=[f\"Topic_{i}_{num_topics}\" for i in range(num_topics)])\n",
        "\n",
        "    # Choose the file path based on the number of topics\n",
        "    file_path = file_paths[num_topics]\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    X_lda.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Results for {num_topics} topics saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_features = pd.read_csv('../Datasets/TFIDF_Features.csv')\n",
        "set_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = set_features  # Assuming 'target_column' is your target column\n",
        "y_unencoded = preprocessed['sentiment']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_unencoded)\n",
        "y = pd.DataFrame(y, columns=['sentiment'])\n",
        "\n",
        "# Define the number of features you want to select\n",
        "k_values = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
        "selected_dfs = {}\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y.to_csv(TARGET_VALUES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. CHI SQUARE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in k_values:\n",
        "    selector = SelectKBest(chi2, k=k).fit(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_chi_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_chi_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_chi_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_chi_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_chi_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_chi_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_chi_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_chi_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_chi_1000.to_csv(CHI_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_1K}\")\n",
        "selected_df_chi_2000.to_csv(CHI_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_2K}\")\n",
        "selected_df_chi_3000.to_csv(CHI_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_3K}\")\n",
        "selected_df_chi_4000.to_csv(CHI_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_4K}\")\n",
        "selected_df_chi_5000.to_csv(CHI_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_5K}\")\n",
        "selected_df_chi_6000.to_csv(CHI_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_6K}\")\n",
        "selected_df_chi_7000.to_csv(CHI_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_7K}\")\n",
        "selected_df_chi_8000.to_csv(CHI_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. CORRELATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming X is your features DataFrame and y is your target DataFrame\n",
        "# Make sure X and y have the same number of rows\n",
        "\n",
        "# Calculate correlation between each feature in X and the target variable in y\n",
        "correlation = X.corrwith(y['sentiment'])\n",
        "\n",
        "# Select the top correlated features for each specified number of features\n",
        "selected_dfs = {}\n",
        "\n",
        "# Sort the correlation values and get the indices of the top correlated features\n",
        "selected_features_1000 = correlation.abs().nlargest(1000).index\n",
        "selected_features_2000 = correlation.abs().nlargest(2000).index\n",
        "selected_features_3000 = correlation.abs().nlargest(3000).index\n",
        "selected_features_4000 = correlation.abs().nlargest(4000).index\n",
        "selected_features_5000 = correlation.abs().nlargest(5000).index\n",
        "selected_features_6000 = correlation.abs().nlargest(6000).index\n",
        "selected_features_7000 = correlation.abs().nlargest(7000).index\n",
        "selected_features_8000 = correlation.abs().nlargest(8000).index\n",
        "\n",
        "# Filter the features dataframe with selected features\n",
        "selected_df_1000 = X[selected_features_1000]\n",
        "selected_df_2000 = X[selected_features_2000]\n",
        "selected_df_3000 = X[selected_features_3000]\n",
        "selected_df_4000 = X[selected_features_4000]\n",
        "selected_df_5000 = X[selected_features_5000]\n",
        "selected_df_6000 = X[selected_features_6000]\n",
        "selected_df_7000 = X[selected_features_7000]\n",
        "selected_df_8000 = X[selected_features_8000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save the selected features to CSV files\n",
        "selected_df_1000.to_csv(CORR_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_1K}\")\n",
        "selected_df_2000.to_csv(CORR_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_2K}\")\n",
        "selected_df_3000.to_csv(CORR_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_3K}\")\n",
        "selected_df_4000.to_csv(CORR_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_4K}\")\n",
        "selected_df_5000.to_csv(CORR_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_5K}\")\n",
        "selected_df_6000.to_csv(CORR_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_6K}\")\n",
        "selected_df_7000.to_csv(CORR_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_7K}\")\n",
        "selected_df_8000.to_csv(CORR_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. INFO GAIN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)  # You can use any integer value as the seed\n",
        "\n",
        "# Initialize a dictionary to store selected DataFrames\n",
        "selected_dfs = {}\n",
        "\n",
        "# Iterate over k_values\n",
        "for k in k_values:\n",
        "    selector = SelectKBest(mutual_info_classif, k=k).fit(X, y['sentiment'])\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_infogain_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_infogain_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_infogain_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_infogain_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_infogain_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_infogain_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_infogain_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_infogain_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save these dataframes to separate files\n",
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_infogain_1000.to_csv(INFOGAIN_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_1K}\")\n",
        "selected_df_infogain_2000.to_csv(INFOGAIN_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_2K}\")\n",
        "selected_df_infogain_3000.to_csv(INFOGAIN_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_3K}\")\n",
        "selected_df_infogain_4000.to_csv(INFOGAIN_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_4K}\")\n",
        "selected_df_infogain_5000.to_csv(INFOGAIN_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_5K}\")\n",
        "selected_df_infogain_6000.to_csv(INFOGAIN_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_6K}\")\n",
        "selected_df_infogain_7000.to_csv(INFOGAIN_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_7K}\")\n",
        "selected_df_infogain_8000.to_csv(INFOGAIN_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Feature Loading & Combinations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 1 : TF-IDF\n",
        "#############################################################################################\n",
        "\n",
        "# Type a : Information Gain\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_info_gain_1k = pd.read_csv(INFOGAIN_TFIDF_1K)\n",
        "tfidf_info_gain_2k = pd.read_csv(INFOGAIN_TFIDF_2K)\n",
        "tfidf_info_gain_3k = pd.read_csv(INFOGAIN_TFIDF_3K)\n",
        "tfidf_info_gain_4k = pd.read_csv(INFOGAIN_TFIDF_4K)\n",
        "tfidf_info_gain_5k = pd.read_csv(INFOGAIN_TFIDF_5K)\n",
        "tfidf_info_gain_6k = pd.read_csv(INFOGAIN_TFIDF_6K)\n",
        "tfidf_info_gain_7k = pd.read_csv(INFOGAIN_TFIDF_7K)\n",
        "tfidf_info_gain_8k = pd.read_csv(INFOGAIN_TFIDF_8K)\n",
        "\n",
        "# Type b : Chi-Square\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_chi_square_1k = pd.read_csv(CHI_TFIDF_1K)\n",
        "tfidf_chi_square_2k = pd.read_csv(CHI_TFIDF_2K)\n",
        "tfidf_chi_square_3k = pd.read_csv(CHI_TFIDF_3K)\n",
        "tfidf_chi_square_4k = pd.read_csv(CHI_TFIDF_4K)\n",
        "tfidf_chi_square_5k = pd.read_csv(CHI_TFIDF_5K)\n",
        "tfidf_chi_square_6k = pd.read_csv(CHI_TFIDF_6K)\n",
        "tfidf_chi_square_7k = pd.read_csv(CHI_TFIDF_7K)\n",
        "tfidf_chi_square_8k = pd.read_csv(CHI_TFIDF_8K)\n",
        "\n",
        "# Type c : Correlation\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_correlation_1k = pd.read_csv(CORR_TFIDF_1K)\n",
        "tfidf_correlation_2k = pd.read_csv(CORR_TFIDF_2K)\n",
        "tfidf_correlation_3k = pd.read_csv(CORR_TFIDF_3K)\n",
        "tfidf_correlation_4k = pd.read_csv(CORR_TFIDF_4K)\n",
        "tfidf_correlation_5k = pd.read_csv(CORR_TFIDF_5K)\n",
        "tfidf_correlation_6k = pd.read_csv(CORR_TFIDF_6K)\n",
        "tfidf_correlation_7k = pd.read_csv(CORR_TFIDF_7K)\n",
        "tfidf_correlation_8k = pd.read_csv(CORR_TFIDF_8K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 2 : Positive and Negative Words Count\n",
        "#############################################################################################\n",
        "\n",
        "pc_nc = pd.read_csv(POSNEG_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 3 : Positive and Negative Connotation Count\n",
        "#############################################################################################\n",
        "\n",
        "pcc_ncc = pd.read_csv(POSNEG_CONN_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 4 : Vader Score\n",
        "#############################################################################################\n",
        "\n",
        "vader_scores = pd.read_csv(VADER_SCORES_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 5 : LDA Topics\n",
        "#############################################################################################\n",
        "\n",
        "lda_150_topics = pd.read_csv(LDA_150)\n",
        "lda_200_topics = pd.read_csv(LDA_200)\n",
        "lda_250_topics = pd.read_csv(LDA_250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Defining Target Values\n",
        "#############################################################################################\n",
        "\n",
        "y = pd.read_csv(TARGET_VALUES)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Release Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables in the current namespace\n",
        "%whos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables that are DataFrames\n",
        "for variable_name, variable_value in globals().items():\n",
        "    if isinstance(variable_value, pd.DataFrame):\n",
        "        print(variable_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use This to Delete Variable\n",
        "# del _\n",
        "# del __\n",
        "# del ___\n",
        "del set_features\n",
        "# del X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TRAIN, TEST AND VAILDATE ON MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# selected_features = tfidf_info_gain_1k\n",
        "# selected_features = tfidf_info_gain_2k\n",
        "# selected_features = tfidf_info_gain_3k\n",
        "# selected_features = tfidf_info_gain_4k\n",
        "# selected_features = tfidf_info_gain_5k\n",
        "# selected_features = tfidf_info_gain_6k\n",
        "# selected_features = tfidf_info_gain_7k\n",
        "# selected_features = tfidf_info_gain_8k\n",
        "\n",
        "# selected_features = tfidf_chi_square_1k\n",
        "# selected_features1 = tfidf_chi_square_2k\n",
        "# selected_features = tfidf_chi_square_3k\n",
        "# selected_features = tfidf_chi_square_4k\n",
        "selected_features1 = tfidf_chi_square_5k\n",
        "# selected_features = tfidf_chi_square_6k\n",
        "# selected_features = tfidf_chi_square_7k\n",
        "# selected_features = tfidf_chi_square_8k\n",
        "\n",
        "# selected_features = tfidf_correlation_1k\n",
        "# selected_features = tfidf_correlation_2k\n",
        "# selected_features = tfidf_correlation_3k\n",
        "# selected_features = tfidf_correlation_4k\n",
        "# selected_features = tfidf_correlation_5k\n",
        "# selected_features = tfidf_correlation_6k\n",
        "# selected_features = tfidf_correlation_7k\n",
        "# selected_features = tfidf_correlation_8k\n",
        "\n",
        "\n",
        "# selected_features1 = pc_nc\n",
        "\n",
        "# selected_features2 = pcc_ncc\n",
        "\n",
        "# selected_features3 = vader_scores\n",
        "\n",
<<<<<<< HEAD
        "# selected_features2 = lda_150_topics\n",
        "# selected_features2 = lda_200_topics\n",
        "selected_features2 = lda_250_topics"
=======
        "# selected_features4 = lda_150_topics\n",
        "# selected_features4 = lda_200_topics\n",
        "# selected_features4 = lda_250_topics"
>>>>>>> b0e5ae7ca8b22aaaa534c64dc0e6f1f235648b34
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming dfs is a list of dataframes\n",
<<<<<<< HEAD
        "selected_features = [selected_features1, selected_features2]  # List of dataframes to be merged\n",
=======
        "selected_features = [selected_features,selected_features1,selected_features2,selected_features3,selected_features4]  # List of dataframes to be merged\n",
>>>>>>> b0e5ae7ca8b22aaaa534c64dc0e6f1f235648b34
        "\n",
        "# Merge dataframes using pd.concat()\n",
        "selected_features = pd.concat(selected_features,axis=1)\n",
        "\n",
        "# Display the merged dataframe\n",
        "selected_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Assuming selected_features is your DataFrame\n",
        "# ''' 'Positive_Connotation_Count', 'Negative_Connotation_Count', \n",
        "#                          'Positive_Word_Count', 'Negative_Word_Count', \n",
        "#                          'Positive_VADER_Count', 'Negative_VADER_Count' '''\n",
        "\n",
        "\n",
<<<<<<< HEAD
        "# columns_to_normalize = ['Positive_Connotation_Count', 'Negative_Connotation_Count',\n",
        "#                         'Positive_VADER_Count', 'Negative_VADER_Count']\n",
=======
        "columns_to_normalize = ['Positive_Connotation_Count', 'Negative_Connotation_Count','Positive_Word_Count', 'Negative_Word_Count',\n",
        "                        'Positive_VADER_Count', 'Negative_VADER_Count']\n",
>>>>>>> b0e5ae7ca8b22aaaa534c64dc0e6f1f235648b34
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# selected_features[columns_to_normalize] = scaler.fit_transform(selected_features[columns_to_normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following code if this \"Unnamed: 0\" column appears \n",
        "# selected_features = selected_features.drop(columns=['Unnamed: 0'])\n",
        "# selected_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **5. Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'Multinomial Naive Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    # Add other classifiers here\n",
        "}\n",
        "\n",
        "# Perform cross-validation predictions and compute classification reports\n",
        "for name, classifier in classifiers.items():\n",
        "    y_pred = cross_val_predict(classifier, selected_features, y['sentiment'], cv=5)\n",
        "    report = classification_report(y['sentiment'], y_pred)\n",
        "    # Print the classification reports\n",
        "    print(f\"Classification Report for {name}:\")\n",
        "    print(report)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5. Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential()\n",
        "# Existing layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
<<<<<<< HEAD
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
=======
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)\n",
>>>>>>> b0e5ae7ca8b22aaaa534c64dc0e6f1f235648b34
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Neural Network:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import cross_val_predict\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Initialize SVM classifier\n",
        "# svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# # Perform cross-validation predictions\n",
        "# y_pred = cross_val_predict(svm_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# # Generate classification report\n",
        "# report = classification_report(y, y_pred)\n",
        "\n",
        "# # Print classification report\n",
        "# print(\"Classification Report:\")\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# # Initialize Logistic Regression classifier\n",
        "# logistic_regression = LogisticRegression()\n",
        "\n",
        "# # Perform cross-validation predictions\n",
        "# y_pred = cross_val_predict(logistic_regression, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# # Generate classification report\n",
        "# report = classification_report(y, y_pred)\n",
        "\n",
        "# # Print classification report\n",
        "# print(\"Classification Report:\")\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# # Initialize KNN classifier with k=5 (you can adjust k as needed)\n",
        "# knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# # Perform cross-validation predictions\n",
        "# y_pred = cross_val_predict(knn_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# # Generate classification report\n",
        "# report = classification_report(y, y_pred)\n",
        "\n",
        "# # Print classification report\n",
        "# print(\"Classification Report:\")\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Multinomial Naive Bayes Classifier\n",
        "# nb_classifier = MultinomialNB()\n",
        "# # Perform cross-validation predictions\n",
        "# y_pred = cross_val_predict(nb_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# # Generate classification report\n",
        "# report = classification_report(y, y_pred)\n",
        "\n",
        "# # Print classification report\n",
        "# print(\"Classification Report:\")\n",
        "# print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
