{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Requirements to be downloaded**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r ../requirements.txt\n",
        "# %pip install kaggle\n",
        "# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p ../Dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Mahesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Mahesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Mahesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Mahesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import zipfile\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from urllib.parse import urlsplit\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perluniprops\n",
            "mwa_ppdb\n",
            "punkt\n",
            "rslp\n",
            "porter_test\n",
            "snowball_data\n",
            "maxent_ne_chunker\n",
            "moses_sample\n",
            "bllip_wsj_no_aux\n",
            "word2vec_sample\n",
            "wmt15_eval\n",
            "spanish_grammars\n",
            "sample_grammars\n",
            "large_grammars\n",
            "book_grammars\n",
            "basque_grammars\n",
            "maxent_treebank_pos_tagger\n",
            "averaged_perceptron_tagger\n",
            "averaged_perceptron_tagger_ru\n",
            "universal_tagset\n",
            "vader_lexicon\n",
            "lin_thesaurus\n",
            "movie_reviews\n",
            "problem_reports\n",
            "pros_cons\n",
            "masc_tagged\n",
            "sentence_polarity\n",
            "webtext\n",
            "nps_chat\n",
            "city_database\n",
            "europarl_raw\n",
            "biocreative_ppi\n",
            "verbnet3\n",
            "pe08\n",
            "pil\n",
            "crubadan\n",
            "gutenberg\n",
            "propbank\n",
            "machado\n",
            "state_union\n",
            "twitter_samples\n",
            "semcor\n",
            "wordnet31\n",
            "extended_omw\n",
            "names\n",
            "ptb\n",
            "nombank.1.0\n",
            "floresta\n",
            "comtrans\n",
            "knbc\n",
            "mac_morpho\n",
            "swadesh\n",
            "rte\n",
            "toolbox\n",
            "jeita\n",
            "product_reviews_1\n",
            "omw\n",
            "wordnet2022\n",
            "sentiwordnet\n",
            "product_reviews_2\n",
            "abc\n",
            "wordnet2021\n",
            "udhr2\n",
            "senseval\n",
            "words\n",
            "framenet_v15\n",
            "unicode_samples\n",
            "kimmo\n",
            "framenet_v17\n",
            "chat80\n",
            "qc\n",
            "inaugural\n",
            "wordnet\n",
            "stopwords\n",
            "verbnet\n",
            "shakespeare\n",
            "ycoe\n",
            "ieer\n",
            "cess_cat\n",
            "switchboard\n",
            "comparative_sentences\n",
            "subjectivity\n",
            "udhr\n",
            "pl196x\n",
            "paradigms\n",
            "gazetteers\n",
            "timit\n",
            "treebank\n",
            "sinica_treebank\n",
            "opinion_lexicon\n",
            "ppattach\n",
            "dependency_treebank\n",
            "reuters\n",
            "genesis\n",
            "cess_esp\n",
            "conll2007\n",
            "nonbreaking_prefixes\n",
            "dolch\n",
            "smultron\n",
            "alpino\n",
            "wordnet_ic\n",
            "brown\n",
            "bcp47\n",
            "panlex_swadesh\n",
            "conll2000\n",
            "universal_treebanks_v20\n",
            "brown_tei\n",
            "cmudict\n",
            "omw-1.4\n",
            "mte_teip5\n",
            "indian\n",
            "conll2002\n",
            "tagsets\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "from urllib.request import urlopen\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Ignore SSL certificate errors\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# NLTK data URL\n",
        "nltk_data_url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml'\n",
        "\n",
        "# Fetch the NLTK data index\n",
        "response = urlopen(nltk_data_url)\n",
        "data = response.read()\n",
        "\n",
        "# Parse the XML data\n",
        "tree = ElementTree.fromstring(data)\n",
        "\n",
        "# Print the NLTK data packages\n",
        "for package in tree.findall('packages/package'):\n",
        "    print(package.attrib['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Set Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths for Local Run\n",
        "IMDB_PREPROCESSED = r\"../Datasets/IMDB_PREPROCESSED_DATASET.csv\"\n",
        "TFIDF_FEATURES = r\"../Datasets/TFIDF_FEATURES_SUBSET.csv\"\n",
        "LEXICON_POSITIVE = r\"../Datasets/positive-words.txt\"\n",
        "LEXICON_NEGATIVE = r\"../Datasets/negative-words.txt\"\n",
        "LEXICON_CONNOTATION = r\"../Datasets/connotations.csv\"\n",
        "POSNEG_CONN_FEATURES = r\"../Datasets/POSNEG_CONN_FEATURES.csv\"\n",
        "POSNEG_FEATURES = r\"../Datasets/POSNEG_FEATURES.csv\"\n",
        "VADER_SCORES_FEATURES = r\"../Datasets/VADER_SCORES_FEATURES.csv\"\n",
        "\n",
        "INFOGAIN_TFIDF_1K = r\"../Datasets/INFOGAIN_TFIDF_1K.csv\"\n",
        "INFOGAIN_TFIDF_2K = r\"../Datasets/INFOGAIN_TFIDF_2K.csv\"\n",
        "INFOGAIN_TFIDF_3K = r\"../Datasets/INFOGAIN_TFIDF_3K.csv\"\n",
        "INFOGAIN_TFIDF_4K = r\"../Datasets/INFOGAIN_TFIDF_4K.csv\"\n",
        "INFOGAIN_TFIDF_5K = r\"../Datasets/INFOGAIN_TFIDF_5K.csv\"\n",
        "INFOGAIN_TFIDF_6K = r\"../Datasets/INFOGAIN_TFIDF_6K.csv\"\n",
        "INFOGAIN_TFIDF_7K = r\"../Datasets/INFOGAIN_TFIDF_7K.csv\"\n",
        "INFOGAIN_TFIDF_8K = r\"../Datasets/INFOGAIN_TFIDF_8K.csv\"\n",
        "\n",
        "CHI_TFIDF_1K = r\"../Datasets/CHI_TFIDF_1K.csv\"\n",
        "CHI_TFIDF_2K = r\"../Datasets/CHI_TFIDF_2K.csv\"\n",
        "CHI_TFIDF_3K = r\"../Datasets/CHI_TFIDF_3K.csv\"\n",
        "CHI_TFIDF_4K = r\"../Datasets/CHI_TFIDF_4K.csv\"\n",
        "CHI_TFIDF_5K = r\"../Datasets/CHI_TFIDF_5K.csv\"\n",
        "CHI_TFIDF_6K = r\"../Datasets/CHI_TFIDF_6K.csv\"\n",
        "CHI_TFIDF_7K = r\"../Datasets/CHI_TFIDF_7K.csv\"\n",
        "CHI_TFIDF_8K = r\"../Datasets/CHI_TFIDF_8K.csv\"\n",
        "\n",
        "CORR_TFIDF_1K = r\"../Datasets/CORR_TFIDF_1K.csv\"\n",
        "CORR_TFIDF_2K = r\"../Datasets/CORR_TFIDF_2K.csv\"\n",
        "CORR_TFIDF_3K = r\"../Datasets/CORR_TFIDF_3K.csv\"\n",
        "CORR_TFIDF_4K = r\"../Datasets/CORR_TFIDF_4K.csv\"\n",
        "CORR_TFIDF_5K = r\"../Datasets/CORR_TFIDF_5K.csv\"\n",
        "CORR_TFIDF_6K = r\"../Datasets/CORR_TFIDF_6K.csv\"\n",
        "CORR_TFIDF_7K = r\"../Datasets/CORR_TFIDF_7K.csv\"\n",
        "CORR_TFIDF_8K = r\"../Datasets/CORR_TFIDF_8K.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataset Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to the ZIP file\n",
        "zip_file_path = r\"../Datasets/imdb-dataset-of-50k-movie-reviews.zip\"\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Find the first file with a .csv extension (assuming it's the one you want)\n",
        "    csv_file = [name for name in zip_ref.namelist() if name.endswith('.csv')][0]\n",
        "    \n",
        "    # Read the CSV file directly from the ZIP archive into a DataFrame\n",
        "    df = pd.read_csv(zip_ref.open(csv_file))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset = df.sample(n=5000,random_state=42).reset_index(drop=True)\n",
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lowercase\n",
        "df_subset[\"review\"]=df_subset[\"review\"].apply(lambda x:x.lower())\n",
        "\n",
        "#Remove punctuations\n",
        "df_subset['review'] = df_subset['review'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "\n",
        "# Remove numbers from the 'reviews' column\n",
        "df_subset['review'] = df_subset['review'].str.replace(r'\\d+', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove stopwords from a text\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'review' column\n",
        "df_subset['review'] = df_subset['review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    # Define a regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Find all matches in the text\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    # Remove URLs from the text\n",
        "    text_without_urls = re.sub(url_pattern, '', text)\n",
        "\n",
        "    return text_without_urls\n",
        "\n",
        "# Example usage\n",
        "df_subset['review'] = df_subset['review'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**HTML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "df_subset[\"review\"] = df_subset[\"review\"].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non-Alphanumeric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extra spaces**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_extra_whitespaces(text):\n",
        "    # Use regular expression to replace multiple whitespaces with a single space\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(remove_extra_whitespaces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Filter non-English comments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_non_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Create a boolean mask for non-English reviews\n",
        "mask = df_subset['review'].apply(filter_non_english)\n",
        "\n",
        "# Create a new DataFrame containing only English reviews\n",
        "df_subset = df_subset[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lemma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to get the part of speech for WordNet lemmatizer\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if the part of speech is not found\n",
        "\n",
        "# Function to lemmatize a text\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply lemmatization to the 'text' column\n",
        "df_subset['review'] = df_subset['review'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LabelEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label = LabelEncoder()\n",
        "df_subset['sentiment'] = label.fit_transform(df_subset['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_subset.to_csv('../Datasets/IMDB_Preprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **2. Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>really liked summerslam due look arena curtain...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>many television show appeal quite many differe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film quickly get major chase scene ever increa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jane austen would definitely approve one br br...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>expectation somewhat high go see movie think s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>one eastwood s best movie separate western goo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>blur childhood memory keep echo cult serie bel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>love zombiemovies love amateurproductions meat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>chan new york get involve attempt sabotage new...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>wife think film watereddown madefortv bbc vers...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4997 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  sentiment\n",
              "0     really liked summerslam due look arena curtain...          1\n",
              "1     many television show appeal quite many differe...          1\n",
              "2     film quickly get major chase scene ever increa...          0\n",
              "3     jane austen would definitely approve one br br...          1\n",
              "4     expectation somewhat high go see movie think s...          0\n",
              "...                                                 ...        ...\n",
              "4992  one eastwood s best movie separate western goo...          1\n",
              "4993  blur childhood memory keep echo cult serie bel...          0\n",
              "4994  love zombiemovies love amateurproductions meat...          0\n",
              "4995  chan new york get involve attempt sabotage new...          1\n",
              "4996  wife think film watereddown madefortv bbc vers...          0\n",
              "\n",
              "[4997 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed = pd.read_csv('../Datasets/IMDB_Preprocessed.csv')\n",
        "preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaa</th>\n",
              "      <th>aaaaaaahhhhhhggg</th>\n",
              "      <th>aaaand</th>\n",
              "      <th>aaaggghhhhhhh</th>\n",
              "      <th>aaagh</th>\n",
              "      <th>aaargh</th>\n",
              "      <th>aaja</th>\n",
              "      <th>aak</th>\n",
              "      <th>aakrosh</th>\n",
              "      <th>...</th>\n",
              "      <th>zuckerman</th>\n",
              "      <th>zucovic</th>\n",
              "      <th>zukor</th>\n",
              "      <th>zukovic</th>\n",
              "      <th>zuniga</th>\n",
              "      <th>zunz</th>\n",
              "      <th>zwart</th>\n",
              "      <th>zwick</th>\n",
              "      <th>zy</th>\n",
              "      <th>zz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 37511 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    aa  aaa  aaaaaaahhhhhhggg  aaaand  aaaggghhhhhhh  aaagh  aaargh  aaja  \\\n",
              "0  0.0  0.0               0.0     0.0            0.0    0.0     0.0   0.0   \n",
              "1  0.0  0.0               0.0     0.0            0.0    0.0     0.0   0.0   \n",
              "2  0.0  0.0               0.0     0.0            0.0    0.0     0.0   0.0   \n",
              "3  0.0  0.0               0.0     0.0            0.0    0.0     0.0   0.0   \n",
              "4  0.0  0.0               0.0     0.0            0.0    0.0     0.0   0.0   \n",
              "\n",
              "   aak  aakrosh  ...  zuckerman  zucovic  zukor  zukovic  zuniga  zunz  zwart  \\\n",
              "0  0.0      0.0  ...        0.0      0.0    0.0      0.0     0.0   0.0    0.0   \n",
              "1  0.0      0.0  ...        0.0      0.0    0.0      0.0     0.0   0.0    0.0   \n",
              "2  0.0      0.0  ...        0.0      0.0    0.0      0.0     0.0   0.0    0.0   \n",
              "3  0.0      0.0  ...        0.0      0.0    0.0      0.0     0.0   0.0    0.0   \n",
              "4  0.0      0.0  ...        0.0      0.0    0.0      0.0     0.0   0.0    0.0   \n",
              "\n",
              "   zwick   zy   zz  \n",
              "0    0.0  0.0  0.0  \n",
              "1    0.0  0.0  0.0  \n",
              "2    0.0  0.0  0.0  \n",
              "3    0.0  0.0  0.0  \n",
              "4    0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 37511 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "totalFeatures = vectorizer.fit_transform(preprocessed['review'])\n",
        "column_names = vectorizer.get_feature_names_out()\n",
        "totalFeatures = pd.DataFrame(totalFeatures.toarray(), columns=column_names)\n",
        "\n",
        "totalFeatures.to_csv('../Datasets/TFIDF_Features.csv', index=False)\n",
        "\n",
        "totalFeatures.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_features = totalFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_features = pd.read_csv('../Datasets/TFIDF_Features.csv')\n",
        "set_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4997 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentiment\n",
              "0             1\n",
              "1             1\n",
              "2             0\n",
              "3             1\n",
              "4             0\n",
              "...         ...\n",
              "4992          1\n",
              "4993          0\n",
              "4994          0\n",
              "4995          1\n",
              "4996          0\n",
              "\n",
              "[4997 rows x 1 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = set_features  # Assuming 'target_column' is your target column\n",
        "y_unencoded = preprocessed['sentiment']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_unencoded)\n",
        "y = pd.DataFrame(y, columns=['sentiment'])\n",
        "\n",
        "# Define the number of features you want to select\n",
        "k_values = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
        "selected_dfs = {}\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. CHI SQUARE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X Shape(4997, 37511)\n",
            "y Shape(4997, 1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in k_values:\n",
        "    selector = SelectKBest(chi2, k=k).fit(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_chi_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_chi_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_chi_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_chi_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_chi_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_chi_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_chi_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_chi_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_chi_1000.to_csv(CHI_TFIDF_1K, index=False)\n",
        "selected_df_chi_2000.to_csv(CHI_TFIDF_2K, index=False)\n",
        "selected_df_chi_3000.to_csv(CHI_TFIDF_3K, index=False)\n",
        "selected_df_chi_4000.to_csv(CHI_TFIDF_4K, index=False)\n",
        "selected_df_chi_5000.to_csv(CHI_TFIDF_5K, index=False)\n",
        "selected_df_chi_6000.to_csv(CHI_TFIDF_6K, index=False)\n",
        "selected_df_chi_7000.to_csv(CHI_TFIDF_7K, index=False)\n",
        "selected_df_chi_8000.to_csv(CHI_TFIDF_8K, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. INFO GAIN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)  # You can use any integer value as the seed\n",
        "\n",
        "# Initialize a dictionary to store selected DataFrames\n",
        "selected_dfs = {}\n",
        "\n",
        "# Iterate over k_values\n",
        "for k in k_values:\n",
        "    selector = SelectKBest(mutual_info_classif, k=k).fit(X, y['OutputSentiment'])\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_infogain_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_infogain_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_infogain_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_infogain_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_infogain_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_infogain_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_infogain_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_infogain_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save these dataframes to separate files\n",
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_infogain_1000.to_csv(INFOGAIN_TFIDF_1K, index=False)\n",
        "selected_df_infogain_2000.to_csv(INFOGAIN_TFIDF_2K, index=False)\n",
        "selected_df_infogain_3000.to_csv(INFOGAIN_TFIDF_3K, index=False)\n",
        "selected_df_infogain_4000.to_csv(INFOGAIN_TFIDF_4K, index=False)\n",
        "selected_df_infogain_5000.to_csv(INFOGAIN_TFIDF_5K, index=False)\n",
        "selected_df_infogain_6000.to_csv(INFOGAIN_TFIDF_6K, index=False)\n",
        "selected_df_infogain_7000.to_csv(INFOGAIN_TFIDF_7K, index=False)\n",
        "selected_df_infogain_8000.to_csv(INFOGAIN_TFIDF_8K, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. CORRELATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X Shape(4997, 37511)\n",
            "y Shape(4997, 1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming X is your features DataFrame and y is your target DataFrame\n",
        "# Make sure X and y have the same number of rows\n",
        "\n",
        "# Calculate correlation between each feature in X and the target variable in y\n",
        "correlation = X.corrwith(y['sentiment'])\n",
        "\n",
        "# Select the top correlated features for each specified number of features\n",
        "selected_dfs = {}\n",
        "\n",
        "# Sort the correlation values and get the indices of the top correlated features\n",
        "selected_features_1000 = correlation.abs().nlargest(1000).index\n",
        "selected_features_2000 = correlation.abs().nlargest(2000).index\n",
        "selected_features_3000 = correlation.abs().nlargest(3000).index\n",
        "selected_features_4000 = correlation.abs().nlargest(4000).index\n",
        "selected_features_5000 = correlation.abs().nlargest(5000).index\n",
        "selected_features_6000 = correlation.abs().nlargest(6000).index\n",
        "selected_features_7000 = correlation.abs().nlargest(7000).index\n",
        "selected_features_8000 = correlation.abs().nlargest(8000).index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the features dataframe with selected features\n",
        "selected_df_1000 = X[selected_features_1000]\n",
        "selected_df_2000 = X[selected_features_2000]\n",
        "selected_df_3000 = X[selected_features_3000]\n",
        "selected_df_4000 = X[selected_features_4000]\n",
        "selected_df_5000 = X[selected_features_5000]\n",
        "selected_df_6000 = X[selected_features_6000]\n",
        "selected_df_7000 = X[selected_features_7000]\n",
        "selected_df_8000 = X[selected_features_8000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save the selected features to CSV files\n",
        "selected_df_1000.to_csv(CORR_TFIDF_1K, index=False)\n",
        "selected_df_2000.to_csv(CORR_TFIDF_2K, index=False)\n",
        "selected_df_3000.to_csv(CORR_TFIDF_3K, index=False)\n",
        "selected_df_4000.to_csv(CORR_TFIDF_4K, index=False)\n",
        "selected_df_5000.to_csv(CORR_TFIDF_5K, index=False)\n",
        "selected_df_6000.to_csv(CORR_TFIDF_6K, index=False)\n",
        "selected_df_7000.to_csv(CORR_TFIDF_7K, index=False)\n",
        "selected_df_8000.to_csv(CORR_TFIDF_8K, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Feature Loading & Combinations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 1 : TF-IDF\n",
        "#############################################################################################\n",
        "\n",
        "# Type a : Information Gain\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_info_gain_1k = pd.read_csv(INFOGAIN_TFIDF_1K)\n",
        "tfidf_info_gain_2k = pd.read_csv(INFOGAIN_TFIDF_2K)\n",
        "tfidf_info_gain_3k = pd.read_csv(INFOGAIN_TFIDF_3K)\n",
        "tfidf_info_gain_4k = pd.read_csv(INFOGAIN_TFIDF_4K)\n",
        "tfidf_info_gain_5k = pd.read_csv(INFOGAIN_TFIDF_5K)\n",
        "tfidf_info_gain_6k = pd.read_csv(INFOGAIN_TFIDF_6K)\n",
        "tfidf_info_gain_7k = pd.read_csv(INFOGAIN_TFIDF_7K)\n",
        "tfidf_info_gain_8k = pd.read_csv(INFOGAIN_TFIDF_8K)\n",
        "\n",
        "# Type b : Chi-Square\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_chi_square_1k = pd.read_csv(CHI_TFIDF_1K)\n",
        "tfidf_chi_square_2k = pd.read_csv(CHI_TFIDF_2K)\n",
        "tfidf_chi_square_3k = pd.read_csv(CHI_TFIDF_3K)\n",
        "tfidf_chi_square_4k = pd.read_csv(CHI_TFIDF_4K)\n",
        "tfidf_chi_square_5k = pd.read_csv(CHI_TFIDF_5K)\n",
        "tfidf_chi_square_6k = pd.read_csv(CHI_TFIDF_6K)\n",
        "tfidf_chi_square_7k = pd.read_csv(CHI_TFIDF_7K)\n",
        "tfidf_chi_square_8k = pd.read_csv(CHI_TFIDF_8K)\n",
        "\n",
        "# Type c : Correlation\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_correlation_1k = pd.read_csv(CORR_TFIDF_1K)\n",
        "tfidf_correlation_2k = pd.read_csv(CORR_TFIDF_2K)\n",
        "tfidf_correlation_3k = pd.read_csv(CORR_TFIDF_3K)\n",
        "tfidf_correlation_4k = pd.read_csv(CORR_TFIDF_4K)\n",
        "tfidf_correlation_5k = pd.read_csv(CORR_TFIDF_5K)\n",
        "tfidf_correlation_6k = pd.read_csv(CORR_TFIDF_6K)\n",
        "tfidf_correlation_7k = pd.read_csv(CORR_TFIDF_7K)\n",
        "tfidf_correlation_8k = pd.read_csv(CORR_TFIDF_8K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 2 : Positive and Negative Words Count\n",
        "#############################################################################################\n",
        "\n",
        "pc_nc = pd.read_csv(POSNEG_FEATURES_SUBSET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 3 : Positive and Negative Connotation Count\n",
        "#############################################################################################\n",
        "\n",
        "pcc_ncc = pd.read_csv(POSNEG_CONN_FEATURES_SUBSET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 4 : Vader Score\n",
        "#############################################################################################\n",
        "\n",
        "vader_scores = pd.read_csv(VADER_SCORES_FEATURES_SUBSET )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Release Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables in the current namespace\n",
        "%whos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables that are DataFrames\n",
        "for variable_name, variable_value in globals().items():\n",
        "    if isinstance(variable_value, pd.DataFrame):\n",
        "        print(variable_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use This to Delete Variable\n",
        "# del _\n",
        "# del __\n",
        "# del ___\n",
        "del set_features\n",
        "# del X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TRAIN, TEST AND VAILDATE ON MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# selected_features = tfidf_info_gain_1k\n",
        "# selected_features = tfidf_info_gain_2k\n",
        "# selected_features = tfidf_info_gain_3k\n",
        "# selected_features = tfidf_info_gain_4k\n",
        "# selected_features = tfidf_info_gain_5k\n",
        "# selected_features = tfidf_info_gain_6k\n",
        "# selected_features = tfidf_info_gain_7k\n",
        "# selected_features = tfidf_info_gain_8k\n",
        "\n",
        "# selected_features = tfidf_chi_square_1k\n",
        "# selected_features = tfidf_chi_square_2k\n",
        "# selected_features = tfidf_chi_square_3k\n",
        "# selected_features = tfidf_chi_square_4k\n",
        "# selected_features = tfidf_chi_square_5k\n",
        "# selected_features = tfidf_chi_square_6k\n",
        "# selected_features = tfidf_chi_square_7k\n",
        "# selected_features = tfidf_chi_square_8k\n",
        "\n",
        "# selected_features = tfidf_correlation_1k\n",
        "# selected_features = tfidf_correlation_2k\n",
        "# selected_features = tfidf_correlation_3k\n",
        "# selected_features = tfidf_correlation_4k\n",
        "# selected_features = tfidf_correlation_5k\n",
        "# selected_features = tfidf_correlation_6k\n",
        "# selected_features = tfidf_correlation_7k\n",
        "# selected_features = tfidf_correlation_8k\n",
        "\n",
        "# selected_features = pc_nc\n",
        "# selected_features = pcc_ncc\n",
        "# selected_features = vader_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming dfs is a list of dataframes\n",
        "selected_features = []  # List of dataframes to be merged\n",
        "\n",
        "# Merge dataframes using pd.concat()\n",
        "selected_features = pd.concat(selected_features, axis=1)\n",
        "\n",
        "# Display the merged dataframe\n",
        "selected_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the following code if this \"Unnamed: 0\" column appears \n",
        "selected_features = selected_features.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "selected_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(svm_classifier, selected_features, y['OutputSentiment'], cv=5)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Initialize Logistic Regression classifier\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(logistic_regression, selected_features, y['OutputSentiment'], cv=5)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Initialize KNN classifier with k=5 (you can adjust k as needed)\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(knn_classifier, selected_features, y['OutputSentiment'], cv=5)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multinomial Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(nb_classifier, selected_features, y['OutputSentiment'], cv=5)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5. Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
