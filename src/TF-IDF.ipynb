{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dependencies to be downloaded**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r ../requirements.txt\n",
        "# %pip install kaggle\n",
        "# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p ../Dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import zipfile\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from urllib.parse import urlsplit\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Set Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths for Local Run\n",
        "IMDB_PREPROCESSED = r\"../Datasets/IMDB_PREPROCESSED.csv\"\n",
        "TFIDF_FEATURES = r\"../Datasets/TFIDF_FEATURES.csv\"\n",
        "LEXICON_POSITIVE = r\"../Datasets/positive-words.txt\"\n",
        "LEXICON_NEGATIVE = r\"../Datasets/negative-words.txt\"\n",
        "LEXICON_CONNOTATION = r\"../Datasets/connotations.csv\"\n",
        "POSNEG_CONN_FEATURES = r\"../Datasets/POSNEG_CONN_FEATURES.csv\"\n",
        "POSNEG_FEATURES = r\"../Datasets/POSNEG_FEATURES.csv\"\n",
        "VADER_SCORES_FEATURES = r\"../Datasets/VADER_SCORES_FEATURES.csv\"\n",
        "TARGET_VALUES = r\"../Datasets/TARGET_VALUES.csv\"\n",
        "\n",
        "INFOGAIN_TFIDF_1K = r\"../Datasets/INFOGAIN_TFIDF_1K.csv\"\n",
        "INFOGAIN_TFIDF_2K = r\"../Datasets/INFOGAIN_TFIDF_2K.csv\"\n",
        "INFOGAIN_TFIDF_3K = r\"../Datasets/INFOGAIN_TFIDF_3K.csv\"\n",
        "INFOGAIN_TFIDF_4K = r\"../Datasets/INFOGAIN_TFIDF_4K.csv\"\n",
        "INFOGAIN_TFIDF_5K = r\"../Datasets/INFOGAIN_TFIDF_5K.csv\"\n",
        "INFOGAIN_TFIDF_6K = r\"../Datasets/INFOGAIN_TFIDF_6K.csv\"\n",
        "INFOGAIN_TFIDF_7K = r\"../Datasets/INFOGAIN_TFIDF_7K.csv\"\n",
        "INFOGAIN_TFIDF_8K = r\"../Datasets/INFOGAIN_TFIDF_8K.csv\"\n",
        "\n",
        "CHI_TFIDF_1K = r\"../Datasets/CHI_TFIDF_1K.csv\"\n",
        "CHI_TFIDF_2K = r\"../Datasets/CHI_TFIDF_2K.csv\"\n",
        "CHI_TFIDF_3K = r\"../Datasets/CHI_TFIDF_3K.csv\"\n",
        "CHI_TFIDF_4K = r\"../Datasets/CHI_TFIDF_4K.csv\"\n",
        "CHI_TFIDF_5K = r\"../Datasets/CHI_TFIDF_5K.csv\"\n",
        "CHI_TFIDF_6K = r\"../Datasets/CHI_TFIDF_6K.csv\"\n",
        "CHI_TFIDF_7K = r\"../Datasets/CHI_TFIDF_7K.csv\"\n",
        "CHI_TFIDF_8K = r\"../Datasets/CHI_TFIDF_8K.csv\"\n",
        "\n",
        "CORR_TFIDF_1K = r\"../Datasets/CORR_TFIDF_1K.csv\"\n",
        "CORR_TFIDF_2K = r\"../Datasets/CORR_TFIDF_2K.csv\"\n",
        "CORR_TFIDF_3K = r\"../Datasets/CORR_TFIDF_3K.csv\"\n",
        "CORR_TFIDF_4K = r\"../Datasets/CORR_TFIDF_4K.csv\"\n",
        "CORR_TFIDF_5K = r\"../Datasets/CORR_TFIDF_5K.csv\"\n",
        "CORR_TFIDF_6K = r\"../Datasets/CORR_TFIDF_6K.csv\"\n",
        "CORR_TFIDF_7K = r\"../Datasets/CORR_TFIDF_7K.csv\"\n",
        "CORR_TFIDF_8K = r\"../Datasets/CORR_TFIDF_8K.csv\"\n",
        "\n",
        "LDA_150 = r\"../Datasets/LDA_150.csv\"\n",
        "LDA_200 = r\"../Datasets/LDA_200.csv\"\n",
        "LDA_250 = r\"../Datasets/LDA_250.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataset Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to the ZIP file\n",
        "zip_file_path = r\"../Datasets/imdb-dataset-of-50k-movie-reviews.zip\"\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Find the first file with a .csv extension (assuming it's the one you want)\n",
        "    csv_file = [name for name in zip_ref.namelist() if name.endswith('.csv')][0]\n",
        "    \n",
        "    # Read the CSV file directly from the ZIP archive into a DataFrame\n",
        "    df = pd.read_csv(zip_ref.open(csv_file))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset = df.sample(n=5000,random_state=42).reset_index(drop=True)\n",
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lowercase\n",
        "df_subset[\"review\"]=df_subset[\"review\"].apply(lambda x:x.lower())\n",
        "\n",
        "#Remove punctuations\n",
        "df_subset['review'] = df_subset['review'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "\n",
        "# Remove numbers from the 'reviews' column\n",
        "df_subset['review'] = df_subset['review'].str.replace(r'\\d+', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove stopwords from a text\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'review' column\n",
        "df_subset['review'] = df_subset['review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    # Define a regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Find all matches in the text\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    # Remove URLs from the text\n",
        "    text_without_urls = re.sub(url_pattern, '', text)\n",
        "\n",
        "    return text_without_urls\n",
        "\n",
        "# Example usage\n",
        "df_subset['review'] = df_subset['review'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**HTML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "df_subset[\"review\"] = df_subset[\"review\"].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non-Alphanumeric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extra spaces**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_extra_whitespaces(text):\n",
        "    # Use regular expression to replace multiple whitespaces with a single space\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df_subset['review'] = df_subset['review'].apply(remove_extra_whitespaces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Filter non-English comments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_non_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Create a boolean mask for non-English reviews\n",
        "mask = df_subset['review'].apply(filter_non_english)\n",
        "\n",
        "# Create a new DataFrame containing only English reviews\n",
        "df_subset = df_subset[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lemma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to get the part of speech for WordNet lemmatizer\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if the part of speech is not found\n",
        "\n",
        "# Function to lemmatize a text\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply lemmatization to the 'text' column\n",
        "df_subset['review'] = df_subset['review'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LabelEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label = LabelEncoder()\n",
        "df_subset['sentiment'] = label.fit_transform(df_subset['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_subset.to_csv('../Datasets/IMDB_Preprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **2. Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = pd.read_csv(IMDB_PREPROCESSED)\n",
        "preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "totalFeatures = vectorizer.fit_transform(preprocessed['review'])\n",
        "column_names = vectorizer.get_feature_names_out()\n",
        "totalFeatures = pd.DataFrame(totalFeatures.toarray(), columns=column_names)\n",
        "\n",
        "totalFeatures.to_csv(TFIDF_FEATURES, index=False)\n",
        "\n",
        "totalFeatures.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Connotations Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connotations = pd.read_csv(LEXICON_CONNOTATION)\n",
        "\n",
        "word_emotion_map = dict(zip(connotations['word'], connotations['emotion']))\n",
        "\n",
        "def update_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'positive')\n",
        "    negative_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'negative')\n",
        "    return positive_count, negative_count\n",
        "\n",
        "pos_neg_conn_counts_df = preprocessed['review'].apply(lambda x: pd.Series(update_counts(x), index=['Positive_Connotation_Count', 'Negative_Connotation_Count']))\n",
        "\n",
        "pos_neg_conn_counts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store the Connotations Count Features\n",
        "pos_neg_conn_counts_df.to_csv(POSNEG_CONN_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Positive and Negative Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load positive and negative words from files\n",
        "positive_words_df = pd.read_csv(LEXICON_POSITIVE, header=None, names=['words'])\n",
        "negative_words_df = pd.read_csv(LEXICON_NEGATIVE, header=None, names=['words'])\n",
        "\n",
        "# Convert DataFrame columns to sets\n",
        "positive_words = set(positive_words_df['words'].tolist())\n",
        "negative_words = set(negative_words_df['words'].tolist())\n",
        "\n",
        "# Define a function to update counts based on positive and negative words\n",
        "def update_word_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in positive_words)\n",
        "    negative_count = sum(1 for word in review.split() if word in negative_words)\n",
        "    return positive_count, negative_count\n",
        "\n",
        "pos_neg_counts_df = preprocessed['review'].apply(lambda x: pd.Series(update_word_counts(x), index=['Positive_Word_Count', 'Negative_Word_Count']))\n",
        "pos_neg_counts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_neg_counts_df.to_csv(POSNEG_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d. Vader Positive Score and Negative Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Use VADER for sentiment analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(review):\n",
        "    scores = sid.polarity_scores(review)\n",
        "    return scores['pos'] *100, scores['neg'] * 100\n",
        "\n",
        "vader_scores_df = preprocessed['review'].apply(lambda x: pd.Series(vader_sentiment(x), index=['Positive_VADER_Count', 'Negative_VADER_Count']))\n",
        "vader_scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vader_scores_df.to_csv(VADER_SCORES_FEATURES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e. Topic Modeling (Latent Dirichlet Allocation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# Define the range of number of topics\n",
        "num_topics_range = [150, 200, 250]  # Add more values if needed\n",
        "\n",
        "# Define the file paths\n",
        "file_paths = {\n",
        "    150: LDA_150,\n",
        "    200: LDA_200,\n",
        "    250: LDA_250\n",
        "}\n",
        "\n",
        "# Loop through different numbers of topics\n",
        "for num_topics in num_topics_range:\n",
        "    \n",
        "    lda_pipeline = make_pipeline(\n",
        "        CountVectorizer(),  # CountVectorizer converts text to a matrix of token counts\n",
        "        TfidfTransformer(),  # TF-IDF transformation\n",
        "        LatentDirichletAllocation(n_components=num_topics, random_state=42)  # LDA for topic modeling\n",
        "    )\n",
        "    \n",
        "    # Fit and transform data using the LDA pipeline\n",
        "    X_lda = lda_pipeline.fit_transform(preprocessed['review'])\n",
        "    \n",
        "    # Create DataFrame for LDA features\n",
        "    X_lda = pd.DataFrame(X_lda, columns=[f\"Topic_{i}_{num_topics}\" for i in range(num_topics)])\n",
        "\n",
        "    # Choose the file path based on the number of topics\n",
        "    file_path = file_paths[num_topics]\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    X_lda.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Results for {num_topics} topics saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_features = pd.read_csv('../Datasets/TFIDF_Features.csv')\n",
        "set_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = set_features  # Assuming 'target_column' is your target column\n",
        "y_unencoded = preprocessed['sentiment']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_unencoded)\n",
        "y = pd.DataFrame(y, columns=['sentiment'])\n",
        "\n",
        "# Define the number of features you want to select\n",
        "k_values = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
        "selected_dfs = {}\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y.to_csv(TARGET_VALUES, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. CHI SQUARE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in k_values:\n",
        "    selector = SelectKBest(chi2, k=k).fit(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_chi_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_chi_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_chi_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_chi_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_chi_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_chi_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_chi_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_chi_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_chi_1000.to_csv(CHI_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_1K}\")\n",
        "selected_df_chi_2000.to_csv(CHI_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_2K}\")\n",
        "selected_df_chi_3000.to_csv(CHI_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_3K}\")\n",
        "selected_df_chi_4000.to_csv(CHI_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_4K}\")\n",
        "selected_df_chi_5000.to_csv(CHI_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_5K}\")\n",
        "selected_df_chi_6000.to_csv(CHI_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_6K}\")\n",
        "selected_df_chi_7000.to_csv(CHI_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_7K}\")\n",
        "selected_df_chi_8000.to_csv(CHI_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {CHI_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. CORRELATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming X is your features DataFrame and y is your target DataFrame\n",
        "# Make sure X and y have the same number of rows\n",
        "\n",
        "# Calculate correlation between each feature in X and the target variable in y\n",
        "correlation = X.corrwith(y['sentiment'])\n",
        "\n",
        "# Select the top correlated features for each specified number of features\n",
        "selected_dfs = {}\n",
        "\n",
        "# Sort the correlation values and get the indices of the top correlated features\n",
        "selected_features_1000 = correlation.abs().nlargest(1000).index\n",
        "selected_features_2000 = correlation.abs().nlargest(2000).index\n",
        "selected_features_3000 = correlation.abs().nlargest(3000).index\n",
        "selected_features_4000 = correlation.abs().nlargest(4000).index\n",
        "selected_features_5000 = correlation.abs().nlargest(5000).index\n",
        "selected_features_6000 = correlation.abs().nlargest(6000).index\n",
        "selected_features_7000 = correlation.abs().nlargest(7000).index\n",
        "selected_features_8000 = correlation.abs().nlargest(8000).index\n",
        "\n",
        "# Filter the features dataframe with selected features\n",
        "selected_df_1000 = X[selected_features_1000]\n",
        "selected_df_2000 = X[selected_features_2000]\n",
        "selected_df_3000 = X[selected_features_3000]\n",
        "selected_df_4000 = X[selected_features_4000]\n",
        "selected_df_5000 = X[selected_features_5000]\n",
        "selected_df_6000 = X[selected_features_6000]\n",
        "selected_df_7000 = X[selected_features_7000]\n",
        "selected_df_8000 = X[selected_features_8000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save the selected features to CSV files\n",
        "selected_df_1000.to_csv(CORR_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_1K}\")\n",
        "selected_df_2000.to_csv(CORR_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_2K}\")\n",
        "selected_df_3000.to_csv(CORR_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_3K}\")\n",
        "selected_df_4000.to_csv(CORR_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_4K}\")\n",
        "selected_df_5000.to_csv(CORR_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_5K}\")\n",
        "selected_df_6000.to_csv(CORR_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_6K}\")\n",
        "selected_df_7000.to_csv(CORR_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_7K}\")\n",
        "selected_df_8000.to_csv(CORR_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {CORR_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. INFO GAIN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"X Shape{X.shape}\")\n",
        "print(f\"y Shape{y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)  # You can use any integer value as the seed\n",
        "\n",
        "# Initialize a dictionary to store selected DataFrames\n",
        "selected_dfs = {}\n",
        "\n",
        "# Iterate over k_values\n",
        "for k in k_values:\n",
        "    selector = SelectKBest(mutual_info_classif, k=k).fit(X, y['sentiment'])\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_indices]\n",
        "    selected_dfs[f'selected_features_{k}'] = set_features[selected_features]\n",
        "\n",
        "# Access the selected dataframes\n",
        "selected_df_infogain_1000 = selected_dfs['selected_features_1000']\n",
        "selected_df_infogain_2000 = selected_dfs['selected_features_2000']\n",
        "selected_df_infogain_3000 = selected_dfs['selected_features_3000']\n",
        "selected_df_infogain_4000 = selected_dfs['selected_features_4000']\n",
        "selected_df_infogain_5000 = selected_dfs['selected_features_5000']\n",
        "selected_df_infogain_6000 = selected_dfs['selected_features_6000']\n",
        "selected_df_infogain_7000 = selected_dfs['selected_features_7000']\n",
        "selected_df_infogain_8000 = selected_dfs['selected_features_8000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, you can save these dataframes to separate files\n",
        "# Save each dataframe to its respective CSV file\n",
        "selected_df_infogain_1000.to_csv(INFOGAIN_TFIDF_1K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_1K}\")\n",
        "selected_df_infogain_2000.to_csv(INFOGAIN_TFIDF_2K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_2K}\")\n",
        "selected_df_infogain_3000.to_csv(INFOGAIN_TFIDF_3K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_3K}\")\n",
        "selected_df_infogain_4000.to_csv(INFOGAIN_TFIDF_4K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_4K}\")\n",
        "selected_df_infogain_5000.to_csv(INFOGAIN_TFIDF_5K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_5K}\")\n",
        "selected_df_infogain_6000.to_csv(INFOGAIN_TFIDF_6K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_6K}\")\n",
        "selected_df_infogain_7000.to_csv(INFOGAIN_TFIDF_7K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_7K}\")\n",
        "selected_df_infogain_8000.to_csv(INFOGAIN_TFIDF_8K, index=False)\n",
        "print(f\"File saved at location: {INFOGAIN_TFIDF_8K}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Feature Loading & Combinations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 1 : TF-IDF\n",
        "#############################################################################################\n",
        "\n",
        "# Type a : Information Gain\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_info_gain_1k = pd.read_csv(INFOGAIN_TFIDF_1K)\n",
        "tfidf_info_gain_2k = pd.read_csv(INFOGAIN_TFIDF_2K)\n",
        "tfidf_info_gain_3k = pd.read_csv(INFOGAIN_TFIDF_3K)\n",
        "tfidf_info_gain_4k = pd.read_csv(INFOGAIN_TFIDF_4K)\n",
        "tfidf_info_gain_5k = pd.read_csv(INFOGAIN_TFIDF_5K)\n",
        "tfidf_info_gain_6k = pd.read_csv(INFOGAIN_TFIDF_6K)\n",
        "tfidf_info_gain_7k = pd.read_csv(INFOGAIN_TFIDF_7K)\n",
        "tfidf_info_gain_8k = pd.read_csv(INFOGAIN_TFIDF_8K)\n",
        "\n",
        "# Type b : Chi-Square\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_chi_square_1k = pd.read_csv(CHI_TFIDF_1K)\n",
        "tfidf_chi_square_2k = pd.read_csv(CHI_TFIDF_2K)\n",
        "tfidf_chi_square_3k = pd.read_csv(CHI_TFIDF_3K)\n",
        "tfidf_chi_square_4k = pd.read_csv(CHI_TFIDF_4K)\n",
        "tfidf_chi_square_5k = pd.read_csv(CHI_TFIDF_5K)\n",
        "tfidf_chi_square_6k = pd.read_csv(CHI_TFIDF_6K)\n",
        "tfidf_chi_square_7k = pd.read_csv(CHI_TFIDF_7K)\n",
        "tfidf_chi_square_8k = pd.read_csv(CHI_TFIDF_8K)\n",
        "\n",
        "# Type c : Correlation\n",
        "# Read CSV files into separate DataFrames\n",
        "tfidf_correlation_1k = pd.read_csv(CORR_TFIDF_1K)\n",
        "tfidf_correlation_2k = pd.read_csv(CORR_TFIDF_2K)\n",
        "tfidf_correlation_3k = pd.read_csv(CORR_TFIDF_3K)\n",
        "tfidf_correlation_4k = pd.read_csv(CORR_TFIDF_4K)\n",
        "tfidf_correlation_5k = pd.read_csv(CORR_TFIDF_5K)\n",
        "tfidf_correlation_6k = pd.read_csv(CORR_TFIDF_6K)\n",
        "tfidf_correlation_7k = pd.read_csv(CORR_TFIDF_7K)\n",
        "tfidf_correlation_8k = pd.read_csv(CORR_TFIDF_8K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 2 : Positive and Negative Words Count\n",
        "#############################################################################################\n",
        "\n",
        "pc_nc = pd.read_csv(POSNEG_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 3 : Positive and Negative Connotation Count\n",
        "#############################################################################################\n",
        "\n",
        "pcc_ncc = pd.read_csv(POSNEG_CONN_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 4 : Vader Score\n",
        "#############################################################################################\n",
        "\n",
        "vader_scores = pd.read_csv(VADER_SCORES_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################################################################################\n",
        "# Feature 5 : LDA Topics\n",
        "#############################################################################################\n",
        "\n",
        "lda_150_topics = pd.read_csv(LDA_150)\n",
        "lda_200_topics = pd.read_csv(LDA_200)\n",
        "lda_250_topics = pd.read_csv(LDA_250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4997 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentiment\n",
              "0             1\n",
              "1             1\n",
              "2             0\n",
              "3             1\n",
              "4             0\n",
              "...         ...\n",
              "4992          1\n",
              "4993          0\n",
              "4994          0\n",
              "4995          1\n",
              "4996          0\n",
              "\n",
              "[4997 rows x 1 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#############################################################################################\n",
        "# Defining Target Values\n",
        "#############################################################################################\n",
        "\n",
        "y = pd.read_csv(TARGET_VALUES)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Release Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables in the current namespace\n",
        "%whos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all variables that are DataFrames\n",
        "for variable_name, variable_value in globals().items():\n",
        "    if isinstance(variable_value, pd.DataFrame):\n",
        "        print(variable_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use This to Delete Variable\n",
        "# del _\n",
        "# del __\n",
        "# del ___\n",
        "del set_features\n",
        "# del X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TRAIN, TEST AND VAILDATE ON MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# selected_features = tfidf_info_gain_1k\n",
        "# selected_features = tfidf_info_gain_2k\n",
        "# selected_features = tfidf_info_gain_3k\n",
        "# selected_features = tfidf_info_gain_4k\n",
        "# selected_features = tfidf_info_gain_5k\n",
        "# selected_features = tfidf_info_gain_6k\n",
        "# selected_features = tfidf_info_gain_7k\n",
        "# selected_features = tfidf_info_gain_8k\n",
        "\n",
        "# selected_features = tfidf_chi_square_1k\n",
        "# selected_features = tfidf_chi_square_2k\n",
        "# selected_features = tfidf_chi_square_3k\n",
        "# selected_features = tfidf_chi_square_4k\n",
        "# selected_features = tfidf_chi_square_5k\n",
        "# selected_features = tfidf_chi_square_6k\n",
        "# selected_features = tfidf_chi_square_7k\n",
        "# selected_features = tfidf_chi_square_8k\n",
        "\n",
        "# selected_features = tfidf_correlation_1k\n",
        "# selected_features = tfidf_correlation_2k\n",
        "# selected_features = tfidf_correlation_3k\n",
        "# selected_features = tfidf_correlation_4k\n",
        "# selected_features = tfidf_correlation_5k\n",
        "# selected_features = tfidf_correlation_6k\n",
        "# selected_features = tfidf_correlation_7k\n",
        "selected_features = tfidf_correlation_8k\n",
        "\n",
        "\n",
        "selected_features1 = pc_nc\n",
        "\n",
        "selected_features2 = pcc_ncc\n",
        "\n",
        "selected_features3 = vader_scores\n",
        "\n",
        "# selected_features = lda_150_topics\n",
        "# selected_features = lda_200_topics\n",
        "selected_features4 = lda_250_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bad</th>\n",
              "      <th>great</th>\n",
              "      <th>waste</th>\n",
              "      <th>nt</th>\n",
              "      <th>awful</th>\n",
              "      <th>best</th>\n",
              "      <th>love</th>\n",
              "      <th>poor</th>\n",
              "      <th>terrible</th>\n",
              "      <th>act</th>\n",
              "      <th>...</th>\n",
              "      <th>Topic_240_250</th>\n",
              "      <th>Topic_241_250</th>\n",
              "      <th>Topic_242_250</th>\n",
              "      <th>Topic_243_250</th>\n",
              "      <th>Topic_244_250</th>\n",
              "      <th>Topic_245_250</th>\n",
              "      <th>Topic_246_250</th>\n",
              "      <th>Topic_247_250</th>\n",
              "      <th>Topic_248_250</th>\n",
              "      <th>Topic_249_250</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.019265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033177</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200227</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.057397</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035752</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.018718</td>\n",
              "      <td>0.047243</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8256 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        bad     great  waste        nt     awful      best  love  poor  \\\n",
              "0  0.000000  0.000000    0.0  0.019265  0.000000  0.033177   0.0   0.0   \n",
              "1  0.000000  0.000000    0.0  0.000000  0.000000  0.000000   0.0   0.0   \n",
              "2  0.057397  0.000000    0.0  0.000000  0.000000  0.000000   0.0   0.0   \n",
              "3  0.000000  0.000000    0.0  0.035752  0.000000  0.061571   0.0   0.0   \n",
              "4  0.000000  0.028222    0.0  0.018718  0.047243  0.000000   0.0   0.0   \n",
              "\n",
              "   terrible  act  ...  Topic_240_250  Topic_241_250  Topic_242_250  \\\n",
              "0  0.200227  0.0  ...       0.000454       0.000454       0.000454   \n",
              "1  0.000000  0.0  ...       0.000341       0.000341       0.000341   \n",
              "2  0.000000  0.0  ...       0.000545       0.000545       0.000545   \n",
              "3  0.000000  0.0  ...       0.000525       0.000525       0.000525   \n",
              "4  0.000000  0.0  ...       0.000399       0.000399       0.000399   \n",
              "\n",
              "   Topic_243_250  Topic_244_250  Topic_245_250  Topic_246_250  Topic_247_250  \\\n",
              "0       0.000454       0.000454       0.000454       0.000454       0.000454   \n",
              "1       0.000341       0.000341       0.000341       0.000341       0.000341   \n",
              "2       0.000545       0.000545       0.000545       0.000545       0.000545   \n",
              "3       0.000525       0.000525       0.000525       0.000525       0.000525   \n",
              "4       0.000399       0.000399       0.000399       0.000399       0.000399   \n",
              "\n",
              "   Topic_248_250  Topic_249_250  \n",
              "0       0.000454       0.000454  \n",
              "1       0.000341       0.000341  \n",
              "2       0.000545       0.000545  \n",
              "3       0.000525       0.000525  \n",
              "4       0.000399       0.000399  \n",
              "\n",
              "[5 rows x 8256 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming dfs is a list of dataframes\n",
        "selected_features = [selected_features,selected_features1,selected_features2,selected_features3,selected_features4]  # List of dataframes to be merged\n",
        "\n",
        "# Merge dataframes using pd.concat()\n",
        "selected_features = pd.concat(selected_features,axis=1)\n",
        "\n",
        "# Display the merged dataframe\n",
        "selected_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming selected_features is your DataFrame and below are the column names which needs to be normalized\n",
        "\n",
        "# 'Positive_Connotation_Count', 'Negative_Connotation_Count','Positive_Word_Count', 'Negative_Word_Count', 'Positive_VADER_Count', 'Negative_VADER_Count'\n",
        "\n",
        "columns_to_normalize = ['Positive_Connotation_Count', 'Negative_Connotation_Count','Positive_Word_Count', 'Negative_Word_Count','Positive_VADER_Count', 'Negative_VADER_Count']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "selected_features[columns_to_normalize] = scaler.fit_transform(selected_features[columns_to_normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following code if this \"Unnamed: 0\" column appears \n",
        "# selected_features = selected_features.drop(columns=['Unnamed: 0'])\n",
        "# selected_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **5. Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87      2479\n",
            "           1       0.86      0.89      0.88      2518\n",
            "\n",
            "    accuracy                           0.88      4997\n",
            "   macro avg       0.88      0.87      0.88      4997\n",
            "weighted avg       0.88      0.88      0.88      4997\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(svm_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 252. MiB for an array with shape (3997, 8256) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m logistic_regression \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform cross-validation predictions\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogistic_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate classification report\u001b[39;00m\n\u001b[0;32m     12\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(y, y_pred)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:1293\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1293\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1306\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1307\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:1378\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1376\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1380\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1201\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1199\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1201\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1281\u001b[0m check_consistent_length(X, y)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 252. MiB for an array with shape (3997, 8256) and data type float64"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Initialize Logistic Regression classifier\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(logistic_regression, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.44      0.51      2479\n",
            "           1       0.57      0.72      0.64      2518\n",
            "\n",
            "    accuracy                           0.58      4997\n",
            "   macro avg       0.59      0.58      0.57      4997\n",
            "weighted avg       0.59      0.58      0.57      4997\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Initialize KNN classifier with k=5 (you can adjust k as needed)\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(knn_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89      2479\n",
            "           1       0.91      0.86      0.89      2518\n",
            "\n",
            "    accuracy                           0.89      4997\n",
            "   macro avg       0.89      0.89      0.89      4997\n",
            "weighted avg       0.89      0.89      0.89      4997\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multinomial Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "# Perform cross-validation predictions\n",
        "y_pred = cross_val_predict(nb_classifier, selected_features, y['sentiment'], cv=5)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5. Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.6925 - loss: 0.6131\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9936 - loss: 0.0258\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0041\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0011\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.0180e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.7637e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3837e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6431e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1209e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.7383e-05\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Neural Network Accuracy on Test Set: 0.942\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential()\n",
        "# Existing layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report for Neural Network:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       498\n",
            "           1       0.95      0.93      0.94       502\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.94      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Neural Network:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 315. MiB for an array with shape (8256, 4997) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assume X_df is your input DataFrame of sequential features (e.g., time series)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# and y_df is your output DataFrame (e.g., labels for binary classification)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert DataFrame to numpy arrays\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mselected_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:12651\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12577\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12580\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12581\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12649\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12650\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1692\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1692\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1725\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1725\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1727\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 315. MiB for an array with shape (8256, 4997) and data type float64"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assume X_df is your input DataFrame of sequential features (e.g., time series)\n",
        "# and y_df is your output DataFrame (e.g., labels for binary classification)\n",
        "\n",
        "# Convert DataFrame to numpy arrays\n",
        "X = selected_features.values\n",
        "y = y.values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Padding sequences if necessary (to ensure fixed length)\n",
        "X_train_padded = pad_sequences(X_train)  # Use padding if sequences have variable lengths\n",
        "X_test_padded = pad_sequences(X_test)    # Use the same padding for test data\n",
        "\n",
        "# Define BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict_classes(X_test_padded)\n",
        "\n",
        "# Convert predictions from probabilities to binary labels\n",
        "y_pred_binary = np.squeeze(y_pred)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred_binary)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'values'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assume X_df is your input DataFrame of sequential features (e.g., time series)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# and y_df is your output DataFrame (e.g., labels for binary classification)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert DataFrame to numpy arrays\u001b[39;00m\n\u001b[0;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m selected_features\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 14\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assume X_df is your input DataFrame of sequential features (e.g., time series)\n",
        "# and y_df is your output DataFrame (e.g., labels for binary classification)\n",
        "\n",
        "# Convert DataFrame to numpy arrays\n",
        "X = selected_features.values\n",
        "y = y.values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Padding sequences if necessary (to ensure fixed length)\n",
        "X_train_padded = pad_sequences(X_train)  # Use padding if sequences have variable lengths\n",
        "X_test_padded = pad_sequences(X_test)    # Use the same padding for test data\n",
        "\n",
        "# Define GRU model\n",
        "model = Sequential()\n",
        "model.add(GRU(64, input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict_classes(X_test_padded)\n",
        "\n",
        "# Convert predictions from probabilities to binary labels\n",
        "y_pred_binary = np.squeeze(y_pred)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred_binary)\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
