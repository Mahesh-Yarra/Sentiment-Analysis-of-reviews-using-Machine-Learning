{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\sdidd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\sdidd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\sdidd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\sdidd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OutputSentiment\n",
              "positive    2519\n",
              "negative    2481\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"../Dataset/IMDB Dataset.csv\")\n",
        "df.head()\n",
        "df = df.rename(columns={'review': 'OriginalReviews'})\n",
        "df = df.rename(columns={'sentiment': 'OutputSentiment'})\n",
        "df_subset = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
        "df_subset.head()\n",
        "df_subset['OutputSentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OriginalReviews</th>\n",
              "      <th>OutputSentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I really liked this Summerslam due to the look...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not many television shows appeal to quite as m...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The film quickly gets to a major chase scene w...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jane Austen would definitely approve of this o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Expectations were somewhat high for me when I ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>One of eastwood's best movies after he had sep...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>My blurred childhood memories have kept the ec...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>I love Zombie-Movies and I love amateur-produc...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>Chan is in New York and he gets involved with ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>My wife and I both thought this film a watered...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        OriginalReviews OutputSentiment\n",
              "0     I really liked this Summerslam due to the look...        positive\n",
              "1     Not many television shows appeal to quite as m...        positive\n",
              "2     The film quickly gets to a major chase scene w...        negative\n",
              "3     Jane Austen would definitely approve of this o...        positive\n",
              "4     Expectations were somewhat high for me when I ...        negative\n",
              "...                                                 ...             ...\n",
              "4995  One of eastwood's best movies after he had sep...        positive\n",
              "4996  My blurred childhood memories have kept the ec...        negative\n",
              "4997  I love Zombie-Movies and I love amateur-produc...        negative\n",
              "4998  Chan is in New York and he gets involved with ...        positive\n",
              "4999  My wife and I both thought this film a watered...        negative\n",
              "\n",
              "[5000 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lowercase\n",
        "df_subset[\"OriginalReviews\"]=df_subset[\"OriginalReviews\"].apply(lambda x:x.lower())\n",
        "\n",
        "def remove_punctuation_from_text(text):\n",
        "    punctuation_to_remove = string.punctuation\n",
        "    translator = str.maketrans(\"\", \"\", punctuation_to_remove)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Assuming df_subset is your DataFrame and 'OriginalReviews' is the column to process\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_punctuation_from_text)\n",
        "\n",
        "# Remove numbers from the 'OriginalReviewss' column\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].str.replace('\\d+', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eastwoods example text\n"
          ]
        }
      ],
      "source": [
        "text = \"eastwood's! example, text\"\n",
        "punctuation_to_remove = string.punctuation\n",
        "translator = str.maketrans(\"\", \"\", punctuation_to_remove)\n",
        "\n",
        "text = text.translate(translator)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove stopwords from a text\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'OriginalReviews' column\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    # Define a regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Find all matches in the text\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    # Remove URLs from the text\n",
        "    text_without_urls = re.sub(url_pattern, '', text)\n",
        "\n",
        "    return text_without_urls\n",
        "\n",
        "# Example usage\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "df_subset[\"OriginalReviews\"] = df_subset[\"OriginalReviews\"].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_extra_whitespaces(text):\n",
        "    # Use regular expression to replace multiple whitespaces with a single space\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_extra_whitespaces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_non_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Create a boolean mask for non-English OriginalReviewss\n",
        "mask = df_subset['OriginalReviews'].apply(filter_non_english)\n",
        "\n",
        "# Create a new DataFrame containing only English OriginalReviewss\n",
        "df_subset = df_subset[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdidd\\AppData\\Local\\Temp\\ipykernel_24384\\486063349.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(lemmatize_text)\n"
          ]
        }
      ],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to get the part of speech for WordNet lemmatizer\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if the part of speech is not found\n",
        "\n",
        "# Function to lemmatize a text\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply lemmatization to the 'text' column\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OriginalReviews</th>\n",
              "      <th>OutputSentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>really liked summerslam due look arena curtain...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>many television show appeal quite many differe...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film quickly get major chase scene ever increa...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jane austen would definitely approve onebr br ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>expectation somewhat high go see movie think s...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>one eastwoods best movie separate western good...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>blur childhood memory keep echo cult serie bel...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>love zombiemovies love amateurproductions meat...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>chan new york get involve attempt sabotage new...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>wife think film watereddown madefortv bbc vers...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4996 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        OriginalReviews OutputSentiment\n",
              "0     really liked summerslam due look arena curtain...        positive\n",
              "1     many television show appeal quite many differe...        positive\n",
              "2     film quickly get major chase scene ever increa...        negative\n",
              "3     jane austen would definitely approve onebr br ...        positive\n",
              "4     expectation somewhat high go see movie think s...        negative\n",
              "...                                                 ...             ...\n",
              "4995  one eastwoods best movie separate western good...        positive\n",
              "4996  blur childhood memory keep echo cult serie bel...        negative\n",
              "4997  love zombiemovies love amateurproductions meat...        negative\n",
              "4998  chan new york get involve attempt sabotage new...        positive\n",
              "4999  wife think film watereddown madefortv bbc vers...        negative\n",
              "\n",
              "[4996 rows x 2 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_subset.to_csv(\"../csv/Preprocessed_data.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction Using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = pd.read_csv('../csv/Preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features_to_keep = 13000\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and SelectKBest\n",
        "pipeline = make_pipeline(TfidfVectorizer(), SelectKBest(f_classif, k=num_features_to_keep))\n",
        "\n",
        "# Fit and transform your data\n",
        "X_transformed = pipeline.fit_transform(preprocessed['OriginalReviews'], preprocessed['OutputSentiment'])\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_feature_names = pipeline.named_steps['tfidfvectorizer'].get_feature_names_out()[pipeline.named_steps['selectkbest'].get_support()]\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "selected_features_df = pd.DataFrame(X_transformed.toarray(), columns=selected_feature_names)\n",
        "\n",
        "# Concatenate the existing DataFrame with the new selected features DataFrame\n",
        "tfidf_df_13k = pd.concat([preprocessed, selected_features_df], axis=1)\n",
        "\n",
        "tfidf_df_13k.head()\n",
        "\n",
        "tfidf_df_13k.to_csv(\"../csv/tfidf_df_13k.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONNOTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\sdidd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the VADER lexicon (run this once)\n",
        "nltk.download('vader_lexicon')\n",
        "delimiter = '\\t'\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "positive = pd.read_csv(r'..\\Connotations\\positive-words.txt', sep=delimiter, names=['words'])\n",
        "negative = pd.read_csv(r'..\\Connotations\\negative-words.txt', sep=delimiter, names=['words'])\n",
        "connotations = pd.read_csv(r\"..\\Connotations\\connotations.csv\")\n",
        "\n",
        "word_emotion_map = dict(zip(connotations['word'], connotations['emotion']))\n",
        "\n",
        "def update_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'positive')\n",
        "    negative_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'negative')\n",
        "    return positive_count, negative_count\n",
        "\n",
        "tfidf_df_13k[['Positive_Connotation_Count', 'Negative_Connotation_Count']] = tfidf_df_13k['OriginalReviews'].apply(update_counts).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load positive and negative words from files\n",
        "positive_words_df = pd.read_csv(r'..\\Connotations\\positive-words.txt', header=None, names=['words'])\n",
        "negative_words_df = pd.read_csv(r'..\\Connotations\\negative-words.txt', header=None, names=['words'])\n",
        "\n",
        "# Convert DataFrame columns to sets\n",
        "positive_words = set(positive_words_df['words'].tolist())\n",
        "negative_words = set(negative_words_df['words'].tolist())\n",
        "\n",
        "# Assuming 'tfidf_df_13k' is your DataFrame\n",
        "# Define a function to update counts based on positive and negative words\n",
        "def update_word_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in positive_words)\n",
        "    negative_count = sum(1 for word in review.split() if word in negative_words)\n",
        "    return positive_count, negative_count\n",
        "\n",
        "# Apply the function to the 'OriginalReviews' column and unpack the result into two new columns\n",
        "tfidf_df_13k[['Positive_Word_Count', 'Negative_Word_Count']] = tfidf_df_13k['OriginalReviews'].apply(update_word_counts).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Use VADER for sentiment analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(review):\n",
        "    scores = sid.polarity_scores(review)\n",
        "    return scores['pos'] *100, scores['neg'] * 100\n",
        "\n",
        "# Apply the function to the 'OriginalReviews' column and unpack the result into two new columns\n",
        "tfidf_df_13k[['Positive_VADER_Count', 'Negative_VADER_Count']] = tfidf_df_13k['OriginalReviews'].apply(vader_sentiment).tolist()\n",
        "\n",
        "tfidf_df_13k.to_csv(\"../csv/tfidf_df_13k_connotations_vader.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = tfidf_df_13k.iloc[4993]['OriginalReviews']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tfidf_df_13k = pd.read_csv(\"../csv/tfidf_df_13k.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 512 bytes for an array with shape (64,) and data type int64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_df_13k_connotations \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../csv/tfidf_df_13k_connotations_vader.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mparsers.pyx:1120\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mparsers.pyx:1222\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mparsers.pyx:1833\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 512 bytes for an array with shape (64,) and data type int64"
          ]
        }
      ],
      "source": [
        "tfidf_df_13k_connotations = pd.read_csv('../csv/tfidf_df_13k_connotations_vader.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PSHjJbnJcoJ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tfidf_df_13k_connotations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_df_13k_connotations \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_df_13k_connotations\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tfidf_df_13k_connotations' is not defined"
          ]
        }
      ],
      "source": [
        "tfidf_df_13k_connotations = tfidf_df_13k_connotations.drop('Unnamed: 0',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_df_13k_connotations = tfidf_df_13k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OriginalReviews</th>\n",
              "      <th>OutputSentiment</th>\n",
              "      <th>007</th>\n",
              "      <th>007s</th>\n",
              "      <th>0080</th>\n",
              "      <th>010</th>\n",
              "      <th>10000</th>\n",
              "      <th>1010</th>\n",
              "      <th>1010br</th>\n",
              "      <th>108</th>\n",
              "      <th>...</th>\n",
              "      <th>zorro</th>\n",
              "      <th>zu</th>\n",
              "      <th>zucco</th>\n",
              "      <th>zuniga</th>\n",
              "      <th>Positive_Connotation_Count</th>\n",
              "      <th>Negative_Connotation_Count</th>\n",
              "      <th>Positive_Word_Count</th>\n",
              "      <th>Negative_Word_Count</th>\n",
              "      <th>Positive_VADER_Count</th>\n",
              "      <th>Negative_VADER_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>really liked summerslam due look arena curtain...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55</td>\n",
              "      <td>24</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>20.3</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>many television show appeal quite many differe...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77</td>\n",
              "      <td>50</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>21.9</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film quickly get major chase scene ever increa...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>23.1</td>\n",
              "      <td>9.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jane austen would definitely approve onebr br ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>32.4</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>expectation somewhat high go see movie think s...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>15.9</td>\n",
              "      <td>14.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4991</th>\n",
              "      <td>one eastwoods best movie separate western good...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>blur childhood memory keep echo cult serie bel...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>17.5</td>\n",
              "      <td>13.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>love zombiemovies love amateurproductions meat...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>32.4</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>chan new york get involve attempt sabotage new...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>38</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>26.6</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>wife think film watereddown madefortv bbc vers...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>14.3</td>\n",
              "      <td>11.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4996 rows × 13008 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        OriginalReviews OutputSentiment  007  \\\n",
              "0     really liked summerslam due look arena curtain...        positive  0.0   \n",
              "1     many television show appeal quite many differe...        positive  0.0   \n",
              "2     film quickly get major chase scene ever increa...        negative  0.0   \n",
              "3     jane austen would definitely approve onebr br ...        positive  0.0   \n",
              "4     expectation somewhat high go see movie think s...        negative  0.0   \n",
              "...                                                 ...             ...  ...   \n",
              "4991  one eastwoods best movie separate western good...        positive  0.0   \n",
              "4992  blur childhood memory keep echo cult serie bel...        negative  0.0   \n",
              "4993  love zombiemovies love amateurproductions meat...        negative  0.0   \n",
              "4994  chan new york get involve attempt sabotage new...        positive  0.0   \n",
              "4995  wife think film watereddown madefortv bbc vers...        negative  0.0   \n",
              "\n",
              "      007s  0080  010  10000  1010  1010br  108  ...  zorro   zu  zucco  \\\n",
              "0      0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "1      0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "2      0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "3      0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "4      0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "...    ...   ...  ...    ...   ...     ...  ...  ...    ...  ...    ...   \n",
              "4991   0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "4992   0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "4993   0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "4994   0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "4995   0.0   0.0  0.0    0.0   0.0     0.0  0.0  ...    0.0  0.0    0.0   \n",
              "\n",
              "      zuniga  Positive_Connotation_Count  Negative_Connotation_Count  \\\n",
              "0        0.0                          55                          24   \n",
              "1        0.0                          77                          50   \n",
              "2        0.0                          20                          18   \n",
              "3        0.0                          22                          18   \n",
              "4        0.0                          69                          42   \n",
              "...      ...                         ...                         ...   \n",
              "4991     0.0                           6                           6   \n",
              "4992     0.0                          29                          17   \n",
              "4993     0.0                          28                          15   \n",
              "4994     0.0                          28                          38   \n",
              "4995     0.0                          28                          28   \n",
              "\n",
              "      Positive_Word_Count  Negative_Word_Count  Positive_VADER_Count  \\\n",
              "0                       7                    9                  20.3   \n",
              "1                      14                    6                  21.9   \n",
              "2                       4                    4                  23.1   \n",
              "3                       7                    5                  32.4   \n",
              "4                       8                   10                  15.9   \n",
              "...                   ...                  ...                   ...   \n",
              "4991                    3                    0                  36.3   \n",
              "4992                    4                    4                  17.5   \n",
              "4993                   10                    2                  32.4   \n",
              "4994                    8                    5                  26.6   \n",
              "4995                    5                    5                  14.3   \n",
              "\n",
              "      Negative_VADER_Count  \n",
              "0                     12.0  \n",
              "1                      2.9  \n",
              "2                      9.1  \n",
              "3                     10.0  \n",
              "4                     14.8  \n",
              "...                    ...  \n",
              "4991                   0.0  \n",
              "4992                  13.2  \n",
              "4993                   4.6  \n",
              "4994                  22.8  \n",
              "4995                  11.2  \n",
              "\n",
              "[4996 rows x 13008 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_df_13k_connotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "uTeIZqyGNoLJ",
        "outputId": "ec0a348c-7c08-45a6-a607-4fe270099f03"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 496. MiB for an array with shape (13000, 4996) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_statistical \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_df_13k_connotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOriginalReviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPositive_Connotation_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNegative_Connotation_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPositive_Word_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNegative_Word_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPositive_VADER_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNegative_VADER_Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_statistical\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4866\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4863\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   4865\u001b[0m bm_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4866\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4869\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4872\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4874\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   4875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 496. MiB for an array with shape (13000, 4996) and data type float64"
          ]
        }
      ],
      "source": [
        "df_statistical = tfidf_df_13k_connotations.drop(columns=['OriginalReviews','Positive_Connotation_Count','Negative_Connotation_Count','Positive_Word_Count','Negative_Word_Count','Positive_VADER_Count','Negative_VADER_Count'], axis=1)\n",
        "df_statistical.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd-FWJp3N_ug"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_statistical' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m label \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m----> 2\u001b[0m df_statistical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutputSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_statistical\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutputSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'df_statistical' is not defined"
          ]
        }
      ],
      "source": [
        "label = LabelEncoder()\n",
        "df_statistical['OutputSentiment'] = label.fit_transform(df_statistical['OutputSentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CHI SQAURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SelectKBest' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This will get the top 5000 relavant features out of the sample\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m chi2_selector \u001b[38;5;241m=\u001b[39m \u001b[43mSelectKBest\u001b[49m(chi2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m df_statistical\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutputSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'SelectKBest' is not defined"
          ]
        }
      ],
      "source": [
        "# # This will get the top 5000 relavant features out of the sample\n",
        "# chi2_selector = SelectKBest(chi2, k=5000)\n",
        "\n",
        "# # This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\n",
        "# X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "# y = df_statistical['OutputSentiment']\n",
        "# X_5000 = chi2_selector.fit_transform(X, y)\n",
        "\n",
        "# # Get the indices of the selected features\n",
        "# selected_feature_indices = chi2_selector.get_support(indices=True)\n",
        "\n",
        "# # Get the names of the selected features\n",
        "# selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "# chisq_5k = X[selected_feature_names]\n",
        "# chisq_5k.head()\n",
        "\n",
        "# chisq_5k = pd.concat([chisq_5k,tfidf_df_13k_connotations.iloc[:, -6:]],axis=1)\n",
        "# chisq_5k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SelectKBest' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This will get the top 5000 relavant features out of the sample\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m chi2_selector \u001b[38;5;241m=\u001b[39m \u001b[43mSelectKBest\u001b[49m(chi2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m df_statistical\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutputSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'SelectKBest' is not defined"
          ]
        }
      ],
      "source": [
        "# # This will get the top 8000 relavant features out of the sample\n",
        "# chi2_selector = SelectKBest(chi2, k=8000)\n",
        "\n",
        "# # This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\n",
        "# X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "# y = df_statistical['OutputSentiment']\n",
        "# X_8000 = chi2_selector.fit_transform(X, y)\n",
        "\n",
        "# # Get the indices of the selected features\n",
        "# selected_feature_indices = chi2_selector.get_support(indices=True)\n",
        "\n",
        "# # Get the names of the selected features\n",
        "# selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "# chisq_8k = X[selected_feature_names]\n",
        "# chisq_8k.head()\n",
        "\n",
        "# chisq_8k = pd.concat([chisq_8k,tfidf_df_13k_connotations.iloc[:, -6:]],axis=1)\n",
        "# chisq_8k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# For 5000 relevant features\n",
        "cor_selector_5k = SelectKBest(f_regression, k=5000)\n",
        "\n",
        "# Transform the dataset to reduce dimensions by considering only the relevant features\n",
        "X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "y = df_statistical['OutputSentiment']\n",
        "X_5000 = cor_selector_5k.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices_5k = cor_selector_5k.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names_5k = X.columns[selected_feature_indices_5k]\n",
        "\n",
        "cor_5k = X[selected_feature_names_5k]\n",
        "cor_5k.head()\n",
        "\n",
        "cor_5k = pd.concat([cor_5k, tfidf_df_13k_connotations.iloc[:, -6:]], axis=1)\n",
        "cor_5k.head()\n",
        "\n",
        "# For 8000 relevant features\n",
        "cor_selector_8k = SelectKBest(f_regression, k=8000)\n",
        "\n",
        "# Transform the dataset to reduce dimensions by considering only the relevant features\n",
        "X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "y = df_statistical['OutputSentiment']\n",
        "X_8000 = cor_selector_8k.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices_8k = cor_selector_8k.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names_8k = X.columns[selected_feature_indices_8k]\n",
        "\n",
        "cor_8k = X[selected_feature_names_8k]\n",
        "cor_8k.head()\n",
        "\n",
        "cor_8k = pd.concat([cor_8k, tfidf_df_13k_connotations.iloc[:, -6:]], axis=1)\n",
        "cor_8k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>007</th>\n",
              "      <th>007s</th>\n",
              "      <th>0080</th>\n",
              "      <th>010</th>\n",
              "      <th>1010</th>\n",
              "      <th>1010br</th>\n",
              "      <th>108</th>\n",
              "      <th>10br</th>\n",
              "      <th>10yearold</th>\n",
              "      <th>110</th>\n",
              "      <th>...</th>\n",
              "      <th>zorina</th>\n",
              "      <th>zorro</th>\n",
              "      <th>zu</th>\n",
              "      <th>zucco</th>\n",
              "      <th>Positive_Connotation_Count</th>\n",
              "      <th>Negative_Connotation_Count</th>\n",
              "      <th>Positive_Word_Count</th>\n",
              "      <th>Negative_Word_Count</th>\n",
              "      <th>Positive_VADER_Count</th>\n",
              "      <th>Negative_VADER_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55</td>\n",
              "      <td>24</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>20.3</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77</td>\n",
              "      <td>50</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>21.9</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>23.1</td>\n",
              "      <td>9.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>32.4</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>15.9</td>\n",
              "      <td>14.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>17.5</td>\n",
              "      <td>13.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>32.4</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>38</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>26.6</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>14.3</td>\n",
              "      <td>11.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4998 rows × 8006 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      007  007s  0080  010  1010  1010br  108  10br  10yearold  110  ...  \\\n",
              "0     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "1     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "2     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "3     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "...   ...   ...   ...  ...   ...     ...  ...   ...        ...  ...  ...   \n",
              "4993  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4994  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4995  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4996  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4997  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "\n",
              "      zorina  zorro   zu  zucco  Positive_Connotation_Count  \\\n",
              "0        0.0    0.0  0.0    0.0                          55   \n",
              "1        0.0    0.0  0.0    0.0                          77   \n",
              "2        0.0    0.0  0.0    0.0                          20   \n",
              "3        0.0    0.0  0.0    0.0                          22   \n",
              "4        0.0    0.0  0.0    0.0                          69   \n",
              "...      ...    ...  ...    ...                         ...   \n",
              "4993     0.0    0.0  0.0    0.0                           6   \n",
              "4994     0.0    0.0  0.0    0.0                          29   \n",
              "4995     0.0    0.0  0.0    0.0                          28   \n",
              "4996     0.0    0.0  0.0    0.0                          28   \n",
              "4997     0.0    0.0  0.0    0.0                          28   \n",
              "\n",
              "      Negative_Connotation_Count  Positive_Word_Count  Negative_Word_Count  \\\n",
              "0                             24                    7                    9   \n",
              "1                             50                   14                    6   \n",
              "2                             18                    4                    4   \n",
              "3                             18                    7                    5   \n",
              "4                             42                    8                   10   \n",
              "...                          ...                  ...                  ...   \n",
              "4993                           6                    3                    0   \n",
              "4994                          17                    4                    4   \n",
              "4995                          15                   10                    2   \n",
              "4996                          38                    8                    5   \n",
              "4997                          28                    5                    5   \n",
              "\n",
              "      Positive_VADER_Count  Negative_VADER_Count  \n",
              "0                     20.3                  12.0  \n",
              "1                     21.9                   2.9  \n",
              "2                     23.1                   9.1  \n",
              "3                     32.4                  10.0  \n",
              "4                     15.9                  14.8  \n",
              "...                    ...                   ...  \n",
              "4993                  36.3                   0.0  \n",
              "4994                  17.5                  13.2  \n",
              "4995                  32.4                   4.6  \n",
              "4996                  26.6                  22.8  \n",
              "4997                  14.3                  11.2  \n",
              "\n",
              "[4998 rows x 8006 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cor_8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming chisq_8k is your DataFrame\n",
        "columns_to_normalize = ['Positive_Connotation_Count', 'Negative_Connotation_Count', \n",
        "                         'Positive_Word_Count', 'Negative_Word_Count', \n",
        "                         'Positive_VADER_Count', 'Negative_VADER_Count']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "cor_8k[columns_to_normalize] = scaler.fit_transform(cor_8k[columns_to_normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>007</th>\n",
              "      <th>007s</th>\n",
              "      <th>0080</th>\n",
              "      <th>010</th>\n",
              "      <th>1010</th>\n",
              "      <th>1010br</th>\n",
              "      <th>108</th>\n",
              "      <th>10br</th>\n",
              "      <th>10yearold</th>\n",
              "      <th>110</th>\n",
              "      <th>...</th>\n",
              "      <th>zorina</th>\n",
              "      <th>zorro</th>\n",
              "      <th>zu</th>\n",
              "      <th>zucco</th>\n",
              "      <th>Positive_Connotation_Count</th>\n",
              "      <th>Negative_Connotation_Count</th>\n",
              "      <th>Positive_Word_Count</th>\n",
              "      <th>Negative_Word_Count</th>\n",
              "      <th>Positive_VADER_Count</th>\n",
              "      <th>Negative_VADER_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.236364</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.104478</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.184332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.336364</td>\n",
              "      <td>0.252525</td>\n",
              "      <td>0.208955</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.290451</td>\n",
              "      <td>0.044547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.077273</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.306366</td>\n",
              "      <td>0.139785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086364</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.104478</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.429708</td>\n",
              "      <td>0.153610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.212121</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.144928</td>\n",
              "      <td>0.210875</td>\n",
              "      <td>0.227343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013636</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481432</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.118182</td>\n",
              "      <td>0.085859</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.232095</td>\n",
              "      <td>0.202765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.113636</td>\n",
              "      <td>0.075758</td>\n",
              "      <td>0.149254</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>0.429708</td>\n",
              "      <td>0.070661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.113636</td>\n",
              "      <td>0.191919</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.352785</td>\n",
              "      <td>0.350230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.113636</td>\n",
              "      <td>0.141414</td>\n",
              "      <td>0.074627</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.189655</td>\n",
              "      <td>0.172043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4998 rows × 8006 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      007  007s  0080  010  1010  1010br  108  10br  10yearold  110  ...  \\\n",
              "0     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "1     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "2     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "3     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4     0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "...   ...   ...   ...  ...   ...     ...  ...   ...        ...  ...  ...   \n",
              "4993  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4994  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4995  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4996  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "4997  0.0   0.0   0.0  0.0   0.0     0.0  0.0   0.0        0.0  0.0  ...   \n",
              "\n",
              "      zorina  zorro   zu  zucco  Positive_Connotation_Count  \\\n",
              "0        0.0    0.0  0.0    0.0                    0.236364   \n",
              "1        0.0    0.0  0.0    0.0                    0.336364   \n",
              "2        0.0    0.0  0.0    0.0                    0.077273   \n",
              "3        0.0    0.0  0.0    0.0                    0.086364   \n",
              "4        0.0    0.0  0.0    0.0                    0.300000   \n",
              "...      ...    ...  ...    ...                         ...   \n",
              "4993     0.0    0.0  0.0    0.0                    0.013636   \n",
              "4994     0.0    0.0  0.0    0.0                    0.118182   \n",
              "4995     0.0    0.0  0.0    0.0                    0.113636   \n",
              "4996     0.0    0.0  0.0    0.0                    0.113636   \n",
              "4997     0.0    0.0  0.0    0.0                    0.113636   \n",
              "\n",
              "      Negative_Connotation_Count  Positive_Word_Count  Negative_Word_Count  \\\n",
              "0                       0.121212             0.104478             0.130435   \n",
              "1                       0.252525             0.208955             0.086957   \n",
              "2                       0.090909             0.059701             0.057971   \n",
              "3                       0.090909             0.104478             0.072464   \n",
              "4                       0.212121             0.119403             0.144928   \n",
              "...                          ...                  ...                  ...   \n",
              "4993                    0.030303             0.044776             0.000000   \n",
              "4994                    0.085859             0.059701             0.057971   \n",
              "4995                    0.075758             0.149254             0.028986   \n",
              "4996                    0.191919             0.119403             0.072464   \n",
              "4997                    0.141414             0.074627             0.072464   \n",
              "\n",
              "      Positive_VADER_Count  Negative_VADER_Count  \n",
              "0                 0.269231              0.184332  \n",
              "1                 0.290451              0.044547  \n",
              "2                 0.306366              0.139785  \n",
              "3                 0.429708              0.153610  \n",
              "4                 0.210875              0.227343  \n",
              "...                    ...                   ...  \n",
              "4993              0.481432              0.000000  \n",
              "4994              0.232095              0.202765  \n",
              "4995              0.429708              0.070661  \n",
              "4996              0.352785              0.350230  \n",
              "4997              0.189655              0.172043  \n",
              "\n",
              "[4998 rows x 8006 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cor_8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes Cross-Validation Scores:\n",
            "[0.907      0.90790791 0.9019019  0.88888889 0.89289289]\n",
            "Mean Accuracy: 0.8997183183183184\n",
            "\n",
            "k-Nearest Neighbors Cross-Validation Scores:\n",
            "[0.702      0.72372372 0.7027027  0.70670671 0.68768769]\n",
            "Mean Accuracy: 0.7045641641641641\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Multinomial Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_scores = cross_val_score(nb_classifier, cor_8k, y, cv=5)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Cross-Validation Scores:\")\n",
        "print(nb_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(nb_scores))\n",
        "\n",
        "# k-Nearest Neighbors Classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_scores = cross_val_score(knn_classifier, cor_5k, y, cv=5)\n",
        "\n",
        "print(\"\\nk-Nearest Neighbors Cross-Validation Scores:\")\n",
        "print(knn_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(knn_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Cross-validation scores: [0.862      0.86686687 0.88688689 0.86886887 0.86886887]\n",
            "Logistic Regression Cross-validation scores: [0.839      0.83483483 0.85885886 0.82882883 0.84384384]\n",
            "SVM Mean Accuracy: 0.8706982982982983\n",
            "SVM Standard Deviation of Accuracy: 0.008474718693444\n",
            "Logistic Regression Mean Accuracy: 0.8410732732732733\n",
            "Logistic Regression Standard Deviation of Accuracy: 0.010170015849325342\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load your data\n",
        "# Assuming X and y are your features and target variables\n",
        "\n",
        "# Initialize models\n",
        "svm_model = SVC(kernel='linear')  # Linear SVM\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation for SVM\n",
        "svm_scores = cross_val_score(svm_model, cor_5k, y, cv=kfold)\n",
        "\n",
        "# Perform 5-fold cross-validation for Logistic Regression\n",
        "logistic_scores = cross_val_score(logistic_model, cor_8k, y, cv=kfold)\n",
        "\n",
        "# Display the cross-validation scores\n",
        "print(\"SVM Cross-validation scores:\", svm_scores)\n",
        "print(\"Logistic Regression Cross-validation scores:\", logistic_scores)\n",
        "\n",
        "# Optionally, you can calculate mean and standard deviation of the scores\n",
        "print(\"SVM Mean Accuracy:\", np.mean(svm_scores))\n",
        "print(\"SVM Standard Deviation of Accuracy:\", np.std(svm_scores))\n",
        "print(\"Logistic Regression Mean Accuracy:\", np.mean(logistic_scores))\n",
        "print(\"Logistic Regression Standard Deviation of Accuracy:\", np.std(logistic_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 10ms/step - loss: 0.3408 - accuracy: 0.8397\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0186 - accuracy: 0.9932\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 7.1343e-04 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 4.1263e-04 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 2.7270e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 1.9434e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 1.4448e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1146e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 8.8492e-05 - accuracy: 1.0000\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "Neural Network Accuracy on Test Set: 0.944\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Assuming chisq_8k has features and y is the output\n",
        "\n",
        "# Encode categorical labels if needed\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(cor_8k, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming chisq_8k has features and y is the output\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Encode categorical labels if needed\u001b[39;00m\n\u001b[0;32m     13\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> 14\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43my\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(chisq_8k, y_encoded, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assuming chisq_8k has features and y is the output\n",
        "\n",
        "# Encode categorical labels if needed\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(cor_5k, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# \n",
        "# \n",
        "# \n",
        "# This is Custom Optimizer\n",
        "# \n",
        "# \n",
        "# \n",
        "\n",
        "optimizer = tf.keras.optimizers.experimental.Adagrad(\n",
        "    learning_rate=0.1,\n",
        "    initial_accumulator_value=0.1,\n",
        "    epsilon=1e-07,\n",
        "    weight_decay=0.001,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    ema_overwrite_frequency=None,\n",
        "    jit_compile=True,\n",
        "    name='Adagrad',\n",
        ")\n",
        "\n",
        "\n",
        "# # Build a simple neural network model\n",
        "# model = \"\"\n",
        "# model = Sequential()\n",
        "# model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train, \n",
        "    epochs=50, batch_size=32, \n",
        "    validation_split=0.15,  # Using a portion of training set for validation\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Neural Network Accuracy on Test Set:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
