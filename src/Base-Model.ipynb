{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: bs4 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 3)) (0.0.2)\n",
            "Requirement already satisfied: langdetect in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: transformers in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 5)) (4.38.2)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 6)) (2.16.0rc0)\n",
            "Requirement already satisfied: torch in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 7)) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 8)) (1.4.1.post1)\n",
            "Requirement already satisfied: nltk in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r ../requirements.txt (line 9)) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bs4->-r ../requirements.txt (line 3)) (4.12.3)\n",
            "Requirement already satisfied: six in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from langdetect->-r ../requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (0.21.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from transformers->-r ../requirements.txt (line 5)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->-r ../requirements.txt (line 5)) (4.66.2)\n",
            "Requirement already satisfied: tensorflow-intel==2.16.0-rc0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow->-r ../requirements.txt (line 6)) (2.16.0rc0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (4.25.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (69.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.0.5)\n",
            "Requirement already satisfied: sympy in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->-r ../requirements.txt (line 7)) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->-r ../requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->-r ../requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->-r ../requirements.txt (line 7)) (2024.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 8)) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: click in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->-r ../requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->-r ../requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->-r ../requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->-r ../requirements.txt (line 5)) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->-r ../requirements.txt (line 5)) (2024.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers->-r ../requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->bs4->-r ../requirements.txt (line 3)) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->-r ../requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->-r ../requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.42.0)\n",
            "Requirement already satisfied: rich in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (13.7.1)\n",
            "Requirement already satisfied: namex in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.0.7)\n",
            "Requirement already satisfied: dm-tree in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.1.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow->-r ../requirements.txt (line 6)) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: kaggle in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.6)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.2.1)\n",
            "Requirement already satisfied: bleach in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\mahesh\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# %pip install -r ../requirements.txt\n",
        "# %pip install kaggle\n",
        "# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p ../Datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''df = pd.read_csv(\"../Dataset/IMDB Dataset.csv\")\n",
        "df.head()\n",
        "df = df.rename(columns={'review': 'OriginalReviews'})\n",
        "df = df.rename(columns={'sentiment': 'OutputSentiment'})\n",
        "df_subset = df.sample(n=5000).reset_index(drop=True)\n",
        "df_subset.head()\n",
        "df_subset['OutputSentiment'].value_counts()\n",
        "\n",
        "def remove_numbers_from_column(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_numbers_from_column)\n",
        "\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(remove_punc)\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(lambda x:[word for word in x.split() if word not in sw]).apply(lambda x:\" \".join(x))\n",
        "\n",
        "# Lemmatization function\n",
        "def lemmatize_column(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply the function to the specific column\n",
        "df_subset['OriginalReviews'] = df_subset['OriginalReviews'].apply(lemmatize_column)\n",
        "\n",
        "df_subset.to_csv(\"../csv/Preprocessed_data.csv\",index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''num_features_to_keep = 13000\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and SelectKBest\n",
        "pipeline = make_pipeline(TfidfVectorizer(), SelectKBest(f_classif, k=num_features_to_keep))\n",
        "\n",
        "# Fit and transform your data\n",
        "X_transformed = pipeline.fit_transform(preprocessed['OriginalReviews'], preprocessed['OutputSentiment'])\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_feature_names = pipeline.named_steps['tfidfvectorizer'].get_feature_names_out()[pipeline.named_steps['selectkbest'].get_support()]\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "selected_features_df = pd.DataFrame(X_transformed.toarray(), columns=selected_feature_names)\n",
        "\n",
        "# Concatenate the existing DataFrame with the new selected features DataFrame\n",
        "tfidf_df_13k = pd.concat([preprocessed, selected_features_df], axis=1)\n",
        "\n",
        "tfidf_df_13k.head()\n",
        "\n",
        "tfidf_df_13k.to_csv(\"../csv/tfidf_df_13k.csv\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONNOTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''delimiter = '\\t'\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "positive = pd.read_csv('../Connotations/positive-words.txt', delimiter,names=['words'])\n",
        "negative = pd.read_csv('../Connotations/negative-words.txt', delimiter,names=['words'])\n",
        "connotations = pd.read_csv(\"../Connotations/connotations.csv\")\n",
        "\n",
        "word_emotion_map = dict(zip(connotations['word'], connotations['emotion']))\n",
        "\n",
        "def update_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'positive')\n",
        "    negative_count = sum(1 for word in review.split() if word in word_emotion_map and word_emotion_map[word] == 'negative')\n",
        "    return positive_count, negative_count\n",
        "\n",
        "tfidf_df_13k[['Positive_Connotation_Count', 'Negative_Connotation_Count']] = tfidf_df_13k['OriginalReviews'].apply(update_counts).tolist()\n",
        "\n",
        "# Load positive and negative words from files\n",
        "positive_words = set(pd.read_csv('../Connotations/positive-words.txt', header=None, squeeze=True).tolist())\n",
        "negative_words = set(pd.read_csv('../Connotations/negative-words.txt', header=None, squeeze=True).tolist())\n",
        "\n",
        "# Assuming 'tfidf_df_13k' is your DataFrame\n",
        "\n",
        "# Define a function to update counts based on positive and negative words\n",
        "def update_word_counts(review):\n",
        "    positive_count = sum(1 for word in review.split() if word in positive_words)\n",
        "    negative_count = sum(1 for word in review.split() if word in negative_words)\n",
        "    return positive_count, negative_count\n",
        "\n",
        "# Apply the function to the 'OriginalReviews' column and unpack the result into two new columns\n",
        "tfidf_df_13k[['Positive_Word_Count', 'Negative_Word_Count']] = tfidf_df_13k['OriginalReviews'].apply(update_word_counts).tolist()\n",
        "tfidf_df_13k.to_csv(\"../csv/tfidf_df_13k_connotations.csv\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = pd.read_csv('../csv/Preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_df_13k = pd.read_csv(\"../csv/tfidf_df_13k.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_df_13k_connotations = pd.read_csv('../csv/tfidf_df_13k_connotations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PSHjJbnJcoJ"
      },
      "outputs": [],
      "source": [
        "tfidf_df_13k_connotations = tfidf_df_13k_connotations.drop('Unnamed: 0',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "uTeIZqyGNoLJ",
        "outputId": "ec0a348c-7c08-45a6-a607-4fe270099f03"
      },
      "outputs": [],
      "source": [
        "df_statistical = tfidf_df_13k_connotations.drop(columns=['OriginalReviews','Positive_Connotation_Count','Negative_Connotation_Count','Positive_Word_Count','Negative_Word_Count'], axis=1)\n",
        "df_statistical.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd-FWJp3N_ug"
      },
      "outputs": [],
      "source": [
        "label = LabelEncoder()\n",
        "df_statistical['OutputSentiment'] = label.fit_transform(df_statistical['OutputSentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CHI SQAURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will get the top 5000 relavant features out of the sample\n",
        "chi2_selector = SelectKBest(chi2, k=5000)\n",
        "\n",
        "# This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\n",
        "X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "y = df_statistical['OutputSentiment']\n",
        "X_5000 = chi2_selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = chi2_selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "chisq_5k = X[selected_feature_names]\n",
        "chisq_5k.head()\n",
        "\n",
        "chisq_5k = pd.concat([chisq_5k,tfidf_df_13k_connotations.iloc[:, -4:]],axis=1)\n",
        "chisq_5k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will get the top 5000 relavant features out of the sample\n",
        "chi2_selector = SelectKBest(chi2, k=8000)\n",
        "\n",
        "# This will transform the dataset i.e, it will reduce the dimensions by just considering the relavant features only\n",
        "X = df_statistical.drop(columns=['OutputSentiment'])\n",
        "y = df_statistical['OutputSentiment']\n",
        "X_8000 = chi2_selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = chi2_selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "chisq_8k = X[selected_feature_names]\n",
        "chisq_8k.head()\n",
        "\n",
        "chisq_8k = pd.concat([chisq_8k,tfidf_df_13k_connotations.iloc[:, -4:]],axis=1)\n",
        "chisq_8k.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CORRELATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "h-jECCHOOM3p",
        "outputId": "43f4d44b-6e9c-421f-cf9b-2029243092df"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'target' is your target variable\n",
        "target_variable = 'OutputSentiment'\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_statistical.corr()\n",
        "\n",
        "# Extract the correlation with the target variable\n",
        "correlation_with_target = correlation_matrix[target_variable].abs()\n",
        "\n",
        "# Select the top 2000 features based on correlation with the target variable\n",
        "top_2000_features = correlation_with_target.nlargest(2000).index\n",
        "\n",
        "corr_2k = df_statistical[top_2000_features]\n",
        "\n",
        "# Display the top 2000 features\n",
        "corr_2k.head()\n",
        "\n",
        "# Select the top 5000 features based on correlation with the target variable\n",
        "top_5000_features = correlation_with_target.nlargest(5000).index\n",
        "\n",
        "corr_5k = df_statistical[top_5000_features]\n",
        "\n",
        "# Display the top 2000 features\n",
        "corr_5k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh6Zx_ciZM6c",
        "outputId": "914d84e9-263e-44e3-80b6-f626d220269a"
      },
      "outputs": [],
      "source": [
        "# Concatenate the DataFrames along the columns axis\n",
        "corr_2k = pd.concat([corr_2k, tfidf_df_13k_connotations.iloc[:, -4:]], axis=1)\n",
        "corr_5k = pd.concat([corr_5k ,tfidf_df_13k_connotations.iloc[:, -4:]], axis=1)\n",
        "\n",
        "if 'OutputSentiment' in corr_2k.columns:\n",
        "    corr_2k = corr_2k.drop('OutputSentiment', axis=1)\n",
        "if 'OutputSentiment' in corr_5k.columns:\n",
        "    corr_5k = corr_5k.drop('OutputSentiment', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CHI SQAURE CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Multinomial Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_scores = cross_val_score(nb_classifier, chisq_8k, y, cv=5)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Cross-Validation Scores:\")\n",
        "print(nb_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(nb_scores))\n",
        "\n",
        "# k-Nearest Neighbors Classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_scores = cross_val_score(knn_classifier, chisq_5k, y, cv=5)\n",
        "\n",
        "print(\"\\nk-Nearest Neighbors Cross-Validation Scores:\")\n",
        "print(knn_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(knn_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load your data\n",
        "# Assuming X and y are your features and target variables\n",
        "\n",
        "# Initialize models\n",
        "svm_model = SVC(kernel='linear')  # Linear SVM\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation for SVM\n",
        "svm_scores = cross_val_score(svm_model, chisq_5k, y, cv=kfold)\n",
        "\n",
        "# Perform 5-fold cross-validation for Logistic Regression\n",
        "logistic_scores = cross_val_score(logistic_model, chisq_8k, y, cv=kfold)\n",
        "\n",
        "# Display the cross-validation scores\n",
        "print(\"SVM Cross-validation scores:\", svm_scores)\n",
        "print(\"Logistic Regression Cross-validation scores:\", logistic_scores)\n",
        "\n",
        "# Optionally, you can calculate mean and standard deviation of the scores\n",
        "print(\"SVM Mean Accuracy:\", np.mean(svm_scores))\n",
        "print(\"SVM Standard Deviation of Accuracy:\", np.std(svm_scores))\n",
        "print(\"Logistic Regression Mean Accuracy:\", np.mean(logistic_scores))\n",
        "print(\"Logistic Regression Standard Deviation of Accuracy:\", np.std(logistic_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CORRELATION CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLbThhjoaVsm",
        "outputId": "6df3fc8b-dc7c-4a15-f829-155c58bb0b0c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Multinomial Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_scores = cross_val_score(nb_classifier, corr_2k, y, cv=5)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Cross-Validation Scores:\")\n",
        "print(nb_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(nb_scores))\n",
        "\n",
        "# k-Nearest Neighbors Classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_scores = cross_val_score(knn_classifier, corr_2k, y, cv=5)\n",
        "\n",
        "print(\"\\nk-Nearest Neighbors Cross-Validation Scores:\")\n",
        "print(knn_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(knn_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsIQOpi_c5mS",
        "outputId": "73373b30-a94c-449f-c7d4-15373fd5d97e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load your data\n",
        "# Assuming X and y are your features and target variables\n",
        "\n",
        "# Initialize models\n",
        "svm_model = SVC(kernel='linear')  # Linear SVM\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation for SVM\n",
        "svm_scores = cross_val_score(svm_model, corr_5k, y, cv=kfold)\n",
        "\n",
        "# Perform 5-fold cross-validation for Logistic Regression\n",
        "logistic_scores = cross_val_score(logistic_model, corr_5k, y, cv=kfold)\n",
        "\n",
        "# Display the cross-validation scores\n",
        "print(\"SVM Cross-validation scores:\", svm_scores)\n",
        "print(\"Logistic Regression Cross-validation scores:\", logistic_scores)\n",
        "\n",
        "# Optionally, you can calculate mean and standard deviation of the scores\n",
        "print(\"SVM Mean Accuracy:\", np.mean(svm_scores))\n",
        "print(\"SVM Standard Deviation of Accuracy:\", np.std(svm_scores))\n",
        "print(\"Logistic Regression Mean Accuracy:\", np.mean(logistic_scores))\n",
        "print(\"Logistic Regression Standard Deviation of Accuracy:\", np.std(logistic_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
