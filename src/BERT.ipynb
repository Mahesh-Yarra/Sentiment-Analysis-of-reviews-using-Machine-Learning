{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8UEb9OHI4gNS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdidd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Load IMDB dataset from CSV\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"../csv/Preprocessed_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Take a subset of 5000 rows\n",
        "subset_df = df.sample(n=50, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    # Remove punctuation using regular expressions\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "subset_df['processed_text'] = subset_df['OriginalReviews'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save the subset to a new CSV file\n",
        "subset_df.to_csv('../Dataset/subset_dataset.csv', index=False)\n",
        "\n",
        "# Assuming your CSV has columns 'review' and 'sentiment'\n",
        "texts = subset_df['OriginalReviews'].values.tolist()\n",
        "labels = subset_df['OutputSentiment'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7390 - accuracy: 0.4500WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002630F3874C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "5/5 [==============================] - 70s 9s/step - loss: 0.7390 - accuracy: 0.4500 - val_loss: 0.6771 - val_accuracy: 0.6000\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 41s 8s/step - loss: 0.6925 - accuracy: 0.4750 - val_loss: 0.6836 - val_accuracy: 0.5000\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 39s 8s/step - loss: 0.6437 - accuracy: 0.6750 - val_loss: 0.7030 - val_accuracy: 0.4000\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026310332DE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 6s 390ms/step\n",
            "Validation Accuracy: 0.4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('bert_sentiment_model\\\\tokenizer_config.json',\n",
              " 'bert_sentiment_model\\\\special_tokens_map.json',\n",
              " 'bert_sentiment_model\\\\vocab.txt',\n",
              " 'bert_sentiment_model\\\\added_tokens.json',\n",
              " 'bert_sentiment_model\\\\tokenizer.json')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'positive': 1, 'negative': 0}\n",
        "train_labels = [label_mapping[label] for label in train_labels]\n",
        "val_labels = [label_mapping[label] for label in val_labels]\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # Assuming binary classification\n",
        "\n",
        "# Tokenize and encode the training and validation data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors='tf')\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256, return_tensors='tf')\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "    tf.constant(train_labels)\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']},\n",
        "    tf.constant(val_labels)\n",
        "))\n",
        "\n",
        "# Set up training parameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset.shuffle(1000).batch(8), epochs=3, validation_data=val_dataset.batch(8))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_predictions = model.predict(val_dataset.batch(8))\n",
        "val_predictions = tf.argmax(val_predictions.logits, axis=1)\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"bert_sentiment_model\")\n",
        "tokenizer.save_pretrained(\"bert_sentiment_model\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
